#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""Event Trace Log IO Parser.

This script performs trace analysis of a single trace and profile recorded with Windows Performance Recorder (or Xperf).

Author:   b g e n e t o @ g m a i l . c o m
History:  v1.0.0 Initial release
          v1.0.1 Updated profile link
Modified: 20230821
Usage:
    $ streamlit run etl-io-parser.py
"""

__VERSION__ = "1.0.0"

import base64
from timeit import default_timer as timer

import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st
from babel.numbers import parse_decimal, parse_number

app_title = "Windows Disk Trace Visualizer"

about = """
Windows Performance Recorder ([WPR](https://learn.microsoft.com/en-us/windows-hardware/test/wpt/windows-performance-recorder))
is a performance recording tool that is based on Event Tracing for Windows ([ETW](https://learn.microsoft.com/en-us/windows/win32/etw/about-event-tracing)).
It records system and application events that you can analyze by parsing the generated Event Trace Log ([ETL](https://learn.microsoft.com/en-us/windows-hardware/drivers/devtest/trace-log)) file.
The ETL file format is a proprietary binary file format used by Microsoft Windows for collecting
and storing event traces generated by various components of the operating system. Since it is a poorly documented and
proprietary format, we need to use the [WPA Exporter](https://learn.microsoft.com/en-us/windows-hardware/test/wpt/exporter)
tool in order to convert the .etl file to a .csv file and then perform the desired trace analysis via csv file. Both WPR and WPA Exporter are part of the
Windows Assessment and Deployment Kit [Windows ADK](https://learn.microsoft.com/en-us/windows-hardware/get-started/adk-install).
"""

summary = f"""
"{app_title}" is a simple but useful web application designed to streamline the process of analyzing Event Trace Log (ETL) files,
specifically focusing on Disk I/O activities. This tool offers an intuitive and user-friendly interface to effortlessly navigate
through complex ETL data, providing insightful summaries and statistics for a thorough understanding of your disk performance requirements.
Key Features:
- **Data Analysis:** Uncover the most frequently requested data size, the percentage of random reading or writing, the average access time, and more.
- **Performance:** Identify the average throughput and IOPS for each disk, and compare the performance of sequential and random requests.
- **Filter Capability:** Filter data by process name or request size.

All this info is valuable for understanding where the performance of your disk/ssd matters most for your desired/traced workload/application.

Version {__VERSION__} made with [Streamlit](https://streamlit.io/) with ❤️ by [bgeneto](https://github.com/bgeneto/).
"""

python_svg = """
<svg width="800px" height="800px" viewBox="0 0 64 64" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M31.885 16c-8.124 0-7.617 3.523-7.617 3.523l.01 3.65h7.752v1.095H21.197S16 23.678 16 31.876c0 8.196 4.537 7.906 4.537 7.906h2.708v-3.804s-.146-4.537 4.465-4.537h7.688s4.32.07 4.32-4.175v-7.019S40.374 16 31.885 16zm-4.275 2.454c.771 0 1.395.624 1.395 1.395s-.624 1.395-1.395 1.395a1.393 1.393 0 0 1-1.395-1.395c0-.771.624-1.395 1.395-1.395z" fill="url(#a)"/><path d="M32.115 47.833c8.124 0 7.617-3.523 7.617-3.523l-.01-3.65H31.97v-1.095h10.832S48 40.155 48 31.958c0-8.197-4.537-7.906-4.537-7.906h-2.708v3.803s.146 4.537-4.465 4.537h-7.688s-4.32-.07-4.32 4.175v7.019s-.656 4.247 7.833 4.247zm4.275-2.454a1.393 1.393 0 0 1-1.395-1.395c0-.77.624-1.394 1.395-1.394s1.395.623 1.395 1.394c0 .772-.624 1.395-1.395 1.395z" fill="url(#b)"/><defs><linearGradient id="a" x1="19.075" y1="18.782" x2="34.898" y2="34.658" gradientUnits="userSpaceOnUse"><stop stop-color="#387EB8"/><stop offset="1" stop-color="#366994"/></linearGradient><linearGradient id="b" x1="28.809" y1="28.882" x2="45.803" y2="45.163" gradientUnits="userSpaceOnUse"><stop stop-color="#FFE052"/><stop offset="1" stop-color="#FFC331"/></linearGradient></defs></svg>
"""
pandas_svg = """
<svg data-name="Layer 1" version="1.1" viewBox="0 0 210.21 280.43" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<metadata><rdf:RDF><cc:Work rdf:about=""><dc:format>image/svg+xml</dc:format><dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage"/></cc:Work></rdf:RDF></metadata>
<defs><style>.cls-1{fill:#130754;}.cls-2{fill:#48e5ac;}.cls-3{fill:#e70488;}</style></defs>
<rect class="cls-1" x="74.51" y="43.03" width="24.09" height="50.02"/>
<rect class="cls-1" x="74.51" y="145.78" width="24.09" height="50.02"/>
<rect class="cls-2" x="74.51" y="107.65" width="24.09" height="23.6" fill="#ffca00"/>
<rect class="cls-1" x="35.81" y="84.15" width="24.09" height="166.27"/>
<rect class="cls-1" x="112.41" y="187.05" width="24.09" height="50.02"/>
<rect class="cls-1" x="112.41" y="84.21" width="24.09" height="50.02"/>
<rect class="cls-3" x="112.41" y="148.84" width="24.09" height="23.6"/>
<rect class="cls-1" x="150.3" y="30" width="24.09" height="166.27"/>
</svg>
"""

usage = """
First, you need to record a trace of your 'Disk I/O activity' with WPRUI.exe. You can trace your Windows boot process or a specific application/workload.
Then you need to convert the saved `.etl` file to a `.csv` file using the `wpaexporter.exe` tool that comes with [Windows ADK](https://learn.microsoft.com/en-us/windows-hardware/get-started/adk-install).
The `wpaexporter.exe` tool is typically located in the `C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit` folder, an example usage follows:
```cmd
> wpaexporter.exe -i boottrace.etl -profile DiskIO.wpaProfile -delimiter ;
```

You can also trace your boot process as follows:

- Open an elevated command prompt and run:

   ```
   > wpr -addboot GeneralProfile.Light -filemode -recordtempto D:\\Temp
   ```

After this, the trace will start automatically at the early stage of the next (re)boot.

- The command syntax to save the boot trace (.etl file) is the following:

   ```
   > wpr -stopboot D:\\Temp\\boottrace.etl
   ```

Now you can upload the (compressed) `.csv` file to this page. The page script &mdash; written in Python{python_svg}using Pandas{pandas_svg} &mdash; will analyze the single trace record using the provided
[profile](https://raw.githubusercontent.com/bgeneto/windows-disk-trace-vis/main/DiskIO.wpaProfile) and show you the results in tabular and graphical formats.
> **Note:** tested with WPR v10.0.25931.1000 on Windows 11 Pro version 22H2.
"""


def render_svg(svg, width="100%", height="100%") -> str:
    """Renders the given svg string."""
    b64 = base64.b64encode(svg.encode("utf-8")).decode("utf-8")
    html = (
        rf'<img width={width} height={height} src="data:image/svg+xml;base64,{b64}"/>'
    )
    return html


@st.cache_data
def read_uploaded_file(uploaded_file) -> pd.DataFrame:
    """Read the uploaded file using pandas."""

    # check if the uploaded file is compressed
    compression = None
    if uploaded_file.name.endswith(".zip"):
        compression = "zip"
    elif uploaded_file.name.endswith(".gz"):
        compression = "gzip"
    elif uploaded_file.name.endswith(".bz2"):
        compression = "bz2"

    # Read the uploaded file using pandas
    data = pd.read_csv(
        uploaded_file,
        sep=";",
        header=None,
        names=column_names,
        dtype=column_types,
        compression=compression,
        skiprows=1,
    )

    # Drop rows where "IO Type" is not "Read" or "Write"
    data = data[data["IO Type"].isin(["Read", "Write"])]

    # Drop rows where "Priority" is not "Normal"
    data = data[data["Priority"] == "Normal"]

    # Check if the "Complete Time (s)" column contains a decimal separator
    # If not, then the decimal separator is a comma
    locale = "en_US"
    if data["Init Time (s)"].str.contains(",").all():
        st.info("Comma (,) detected as decimal separator.", icon="ℹ️")
        locale = "de_DE"

    # Convert time columns to float using babel library
    data["Init Time (s)"] = (
        data["Init Time (s)"]
        .apply(parse_decimal, locale=locale, strict=False)
        .astype(float)
    )

    data["Complete Time (s)"] = (
        data["Complete Time (s)"]
        .apply(parse_decimal, locale=locale, strict=False)
        .astype(float)
    )

    data["Disk Service Time (µs)"] = (
        data["Disk Service Time (µs)"]
        .apply(parse_decimal, locale=locale, strict=False)
        .astype(float)
    )

    data["IO Time (µs)"] = (
        data["IO Time (µs)"]
        .apply(parse_decimal, locale=locale, strict=False)
        .astype(float)
    )

    # Order the DataFrame by "Init Time (s)" column in order to calculate the difference between max offsets and min offsets
    # and categorize the operation type as "SEQ" or "RND"
    data = data.sort_values(by=["Init Time (s)"], ignore_index=True)

    # Convert "Size (B)" column to int by removing the "."
    data["Size (B)"] = data["Size (B)"].apply(parse_number, locale=locale).astype(int)

    # Convert "Disks" to string so plotly can recognize it as categorical (not numerical, continuous)
    data["Disks"] = "Disk " + data["Disks"].astype(str)

    # Convert "Min Offset" and "Max Offset" from hex to int
    data["Min Offset"] = data["Min Offset"].apply(lambda x: int(x, 16))
    data["Max Offset"] = data["Max Offset"].apply(lambda x: int(x, 16))

    # Classify the operation type as "SEQ" or "RND" by comparing the "Max Offset" from previous row with the "Min Offset" from current row
    # Calculate the difference between max offsets and min offsets
    data["Category"] = np.where(
        data["Min Offset"] == data["Max Offset"].shift(1) + 1, "SEQ", "RND"
    )

    return data


@st.cache_data
def update_disks_names(data: pd.DataFrame, names: dict) -> pd.DataFrame:
    """Change disk name."""
    for old_name, new_name in names.items():
        data.loc[data["Disks"] == old_name, "Disks"] = new_name

    return data


@st.cache_data
def log_summary(data: pd.DataFrame, process_name: str = "") -> pd.DataFrame:
    """Show totals."""

    log_summary = {}

    # Compute the total monitoring time by taking the max value from the "Time" column
    log_summary["Monitoring time"] = "{:.2g} seconds".format(
        data["Complete Time (s)"].max()
    )

    # average time for each operation
    log_summary["Average access time"] = "{:.2f} µs".format(
        data["Disk Service Time (µs)"].mean()
    )

    # number of read and write requests
    log_summary["Read requests"] = data[data["IO Type"] == "Read"].shape[0]
    log_summary["Write requests"] = data[data["IO Type"] == "Write"].shape[0]

    # total requests
    log_summary["Total requests"] = (
        log_summary["Read requests"] + log_summary["Write requests"]
    )

    # percentage of read and write requests
    log_summary["Percent READ"] = "{:.2f}%".format(
        log_summary["Read requests"] / log_summary["Total requests"] * 100
    )
    log_summary["Percent WRITE"] = "{:.2f}%".format(
        log_summary["Write requests"] / log_summary["Total requests"] * 100
    )

    # total percentage SEQ and RND requests
    log_summary["Percent RANDOM"] = "{:.2f}%".format(
        data[data["Category"] == "RND"].shape[0] / log_summary["Total requests"] * 100
    )
    log_summary["Percent SEQUENTIAL"] = "{:.2f}%".format(
        data[data["Category"] == "SEQ"].shape[0] / log_summary["Total requests"] * 100
    )

    # total data read in GBytes
    log_summary["Read data size"] = "{:.2f} GB".format(
        (data["Count"] * data[data["IO Type"] == "Read"]["Size (B)"]).sum() / toGB
    )

    # total data written in GBytes
    log_summary["Write data size"] = "{:.2f} GB".format(
        (data["Count"] * data[data["IO Type"] == "Write"]["Size (B)"]).sum() / toGB
    )

    # total data size in GBytes
    log_summary["Total data size"] = "{:.2f} GB".format(
        (data["Count"] * data["Size (B)"]).sum() / toGB
    )

    # min and max readrequests in KB
    log_summary["Min. read request size"] = "{:.1f} KB".format(
        (data["Count"] * data[data["IO Type"] == "Read"]["Size (B)"]).min() / toKB
    )

    # avg read request in KB
    log_summary["Avg. read request size"] = "{:.1f} KB".format(
        (data["Count"] * data[data["IO Type"] == "Read"]["Size (B)"]).mean() / toKB
    )

    # max read requests in KB
    number = (data["Count"] * data[data["IO Type"] == "Read"]["Size (B)"]).max() / toKB
    formatted_number = (
        "{:.1f}".format(number) if number % 1 else "{:.0f}".format(number)
    )
    log_summary["Max. read request size"] = formatted_number + " KB"

    # min and max write requests in KB
    log_summary["Min. write request size"] = "{:.1f} KB".format(
        (data["Count"] * data[data["IO Type"] == "Write"]["Size (B)"]).min() / toKB
    )

    # avg write request in KB
    log_summary["Avg. write request size"] = "{:.1f} KB".format(
        (data["Count"] * data[data["IO Type"] == "Write"]["Size (B)"]).mean() / toKB
    )

    # max write requests in KB
    number = (data["Count"] * data[data["IO Type"] == "Write"]["Size (B)"]).max() / toKB
    formatted_number = (
        "{:.1f}".format(number) if number % 1 else "{:.0f}".format(number)
    )
    log_summary["Max. write request size"] = formatted_number + " KB"

    return pd.DataFrame.from_dict(log_summary, orient="index", columns=["Value"])


def plot_summary(data: pd.DataFrame):
    # Total requests in GB
    # --------------------------------------
    df = (
        (data.groupby(["Disks", "IO Type"])["Size (B)"].sum() / toGB)
        .to_frame()
        .reset_index()
    )

    # Calculate percentage within each disk group
    df["Percent"] = (
        df["Size (B)"] / df.groupby("Disks")["Size (B)"].transform("sum") * 100
    )

    df.rename(columns={"Size (B)": "Size (GB)"}, inplace=True)

    # Create a plotly bar chart
    fig = px.bar(
        df,
        x="Disks",
        y="Size (GB)",
        title="Requested Data Size",
        color="IO Type",
        barmode="group",
        text="Percent",
    )

    # Annotate the bars with percentage values
    fig.update_traces(texttemplate="%{text:.3s}%", textposition="inside")

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)
        st.write(
            '> **Note:** The percentage displayed on this chart is distinct from the "Percent READ" and "Percent WRITE" indicated in '
            + "the summary table above. This distinction arises because the table presents the percentage of requests, whereas the chart illustrates "
            + "the percentage of data size. Because the size (in Bytes) of each request can vary, numerous (small length) requests may lead to a higher number of requests "
            + "but lower amount of data being read or written."
        )

    # RND and SEQ requested data size in GB
    # --------------------------------------
    df = (
        (data.groupby(["Disks", "Category"])["Size (B)"].sum() / toGB)
        .to_frame()
        .reset_index()
    )

    # Calculate percentage within each disk group
    df["Percent"] = (
        df["Size (B)"] / df.groupby("Disks")["Size (B)"].transform("sum") * 100
    )

    df.rename(columns={"Size (B)": "Size (GB)"}, inplace=True)

    # Create a plotly bar chart
    fig = px.bar(
        df,
        x="Disks",
        y="Size (GB)",
        title="Requested Data Size by Category (Random/Sequential)",
        color="Category",
        barmode="group",
        text="Percent",
    )

    # Annotate the bars with percentage values
    fig.update_traces(texttemplate="%{text:.3s}%", textposition="inside")

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # RND and SEQ requested data by request type per disk
    # --------------------------------------
    disks_names = sorted(data["Disks"].unique().tolist())
    for disk_name in disks_names:
        df = (
            (
                data[data["Disks"] == disk_name]
                .groupby(["IO Type", "Category"])["Size (B)"]
                .sum()
                / toGB
            )
            .to_frame()
            .reset_index()
        )

        # Calculate percentage within each disk group
        df["Percent"] = (
            df["Size (B)"] / df.groupby("Category")["Size (B)"].transform("sum") * 100
        )

        df.rename(columns={"Size (B)": "Size (GB)"}, inplace=True)

        # Create a plotly bar chart
        fig = px.bar(
            df,
            x="Category",
            y="Size (GB)",
            title=f"Requested Data Size by Category (RND/SEQ) and IO Type (R/W) - {disk_name}",
            color="IO Type",
            barmode="group",
            text="Percent",
        )

        # Annotate the bars with percentage values
        fig.update_traces(texttemplate="%{text:.3s}%", textposition="inside")

        st.plotly_chart(fig, use_container_width=True)

        with st.expander("Show data"):
            st.dataframe(df)


def outlier_thresholds_iqr(data: pd.DataFrame, col_name: str, lth=0.05, uth=0.95):
    """
    Calculates the lower and upper outlier thresholds using the interquartile range (IQR) method.

    Args:
        data (pd.DataFrame): The input DataFrame.
        col_name (str): The column name from which to calculate the thresholds.
        th1 (float): The lower percentile for the quartile calculation. Defaults to 0.05.
        th3 (float): The upper percentile for the quartile calculation. Defaults to 0.95.

    Returns:
        float: The lower outlier threshold.
        float: The upper outlier threshold.
    """
    quartile1 = data[col_name].quantile(lth)
    quartile3 = data[col_name].quantile(uth)
    iqr = quartile3 - quartile1
    upper_limit = quartile3 + 1.5 * iqr
    lower_limit = quartile1 - 1.5 * iqr
    return lower_limit, upper_limit


def show_performance(data: pd.DataFrame, filter_size, remove_outliers: bool = False):
    """Compute disk access time and other performance metrics."""
    # Remove outliers
    fdf = data
    if remove_outliers:
        _, ub = outlier_thresholds_iqr(data, "Disk Service Time (µs)", 0.01, 0.99)

        # Filter the DataFrame based on quartiles
        fdf = data[
            (data["Disk Service Time (µs)"] > 0)
            & (data["Disk Service Time (µs)"] <= ub)
        ]

    # average access time by disk
    df = fdf.groupby(["Disks"])[
        "Disk Service Time (µs)"
    ].mean()  # Reset index to access "Disks" as a column

    # Create Plotly bar plot
    fig = px.bar(
        df.reset_index(),
        x="Disks",
        y="Disk Service Time (µs)",
        title="Average Access Time",
        color="Disks",
        labels={"Disk Service Time (µs)": "Average Access Time (µs)"},
    )

    # Set x-axis tickmode to "array" and provide the index values to only show those
    fig.update_xaxes(tickmode="array", tickvals=df.index)

    # Add unit to hover text using hovertemplate
    fig.update_traces(
        hovertemplate="Disk: %{x}<br>Average Time: %{y:.2f} µs<extra></extra>"
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average by disk and by request type
    df = fdf.groupby(["Disks", "IO Type"])["Disk Service Time (µs)"].mean()

    # Create an interactive grouped bar plot using Plotly
    fig = px.bar(
        df.unstack(),
        color="IO Type",
        barmode="group",
        title="Average Access Time per IO Type",
        labels={"value": "Average Access Time (µs)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average throughput by disk
    total_time_sec = fdf.groupby(["Disks"])["Disk Service Time (µs)"].sum() * 1e-6
    df = (fdf.groupby(["Disks"])["Size (B)"].sum() / toMB) / total_time_sec
    df.rename("Average Throughput (MB/s)", inplace=True)

    fig = px.bar(
        df.reset_index(),
        x="Disks",
        y="Average Throughput (MB/s)",
        color="Disks",
        title="Average Throughput",
        labels={"value": "Average Throughput (MB/s)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average throughput by disk and by IO type
    total_time_sec = (
        fdf.groupby(["Disks", "IO Type"])["Disk Service Time (µs)"].sum() * 1e-6
    )
    df = (fdf.groupby(["Disks", "IO Type"])["Size (B)"].sum() / toMB) / total_time_sec

    # Create an interactive grouped bar plot using Plotly
    fig = px.bar(
        df.unstack(),
        color="IO Type",
        barmode="group",
        title="Average Throughput per IO Type",
        labels={"value": "Average Throughput (MB/s)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average throughput per category
    total_time_sec = (
        fdf.groupby(["Disks", "Category"])["Disk Service Time (µs)"].sum() * 1e-6
    )
    df = (fdf.groupby(["Disks", "Category"])["Size (B)"].sum() / toMB) / total_time_sec

    # Create an interactive grouped bar plot using Plotly
    fig = px.bar(
        df.unstack(),
        color="Category",
        barmode="group",
        title="Average Throughput per Category",
        labels={"value": "Average Throughput (MB/s)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average IOPS
    total_time_sec = (
        fdf.groupby(["Disks", "IO Type"])["Disk Service Time (µs)"].sum() * 1e-6
    )
    df = fdf.groupby(["Disks", "IO Type"])["Size (B)"].count() / total_time_sec

    # Create an interactive grouped bar plot using Plotly
    fig = px.bar(
        df.unstack(),
        color="IO Type",
        barmode="group",
        title=f"Average IOPS (size: {filter_size} KB)",
        labels={"value": "Average IOPS"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)


def show_request_size(data: pd.DataFrame):
    # compute average read and write request size in kbytes
    df = (
        (data.groupby(["Disks", "IO Type"])["Size (B)"].mean() / toKB)
        .to_frame()
        .reset_index()
    )

    # rename the columns
    df.rename(columns={"Size (B)": "Avg. Size (KB)"}, inplace=True)

    # Create Plotly bar plot
    fig = px.bar(
        df,
        x="Disks",
        y="Avg. Size (KB)",
        color="IO Type",
        barmode="group",
        title="Average Request Size",
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # Define specific "Size (B)" values to count
    length_values = [
        512,
        1024,
        2048,
        4096,
        8192,
        16384,
        32768,
        65536,
        131072,
        262144,
        524288,
        1048576,
        2097152,
        4194304,
    ]

    # Count occurrences of specific "Size (B)"
    length_counts = (
        data[data["Size (B)"].isin(length_values)]
        .groupby(["Disks", "IO Type"])["Size (B)"]
        .value_counts()
        .sort_index()
        .to_frame()
        .reset_index()
    )

    for disk_name in length_counts["Disks"].unique():
        df = length_counts[length_counts["Disks"] == disk_name]
        df["Request Size (KB)"] = (df["Size (B)"] / toKB).astype(int)
        df.drop(["Disks", "Size (B)"], axis=1, inplace=True)
        # Calculate percentage within each request group
        df["Percent"] = (
            df["count"] / df.groupby("IO Type")["count"].transform("sum") * 100
        )
        # Create the plot
        fig = px.bar(
            df,
            x="Request Size (KB)",
            y="count",
            color="IO Type",
            barmode="group",
            title=f"Number of Requests by Size - {disk_name}",
            text="Percent",
            custom_data=["IO Type", "Percent"],
        )
        # Annotate the bars with percentage values
        fig.update_xaxes(type="category")
        fig.update_traces(
            texttemplate="%{text:.3s}%",
            textposition="inside",
            hovertemplate="Size: %{x}KB<br>Count: %{y}<br>Type: %{customdata[0]}<br>Percent: %{customdata[1]:.1f}%<extra></extra>",
        )
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("Show data"):
            st.dataframe(df)


def initial_sidebar_config():
    # sidebar contents
    sidebar = st.sidebar
    sidebar.subheader("..:: MENU ::..")
    return sidebar


def reset_filters():
    st.session_state.process_name = "ALL"
    st.session_state.avail_sizes = "ALL"


# Define the Streamlit app
def main():
    st.set_page_config(
        page_title=app_title,
        # layout="wide",
        page_icon=":chart_with_upwards_trend:",
        initial_sidebar_state="auto",
    )

    # Set the page title
    st.title(f":chart_with_upwards_trend: {app_title}")

    # Set the sidebar contents
    sidebar = initial_sidebar_config()

    with st.expander("About this page"):
        st.header("About this page")
        st.write(summary)

    with st.expander("About WPR"):
        st.header("About WPR")
        st.write(about)

    with st.expander("Usage"):
        st.header(":computer: Usage")
        st.write(
            usage.format(
                python_svg=render_svg(python_svg, width="4%"),
                pandas_svg=render_svg(pandas_svg, width="2%"),
            ),
            unsafe_allow_html=True,
        )

    st.header(":outbox_tray: Upload your log file")
    uploaded_file = st.file_uploader(
        "Upload your (compressed) wpaexporter generated csv file:",
        type=["csv", "zip", "gz", "bz2"],
        key="uploaded_file",
    )

    if uploaded_file is None:
        st.warning(
            "You have to upload your log file before we can continue...", icon="⚠️"
        )
        return

    # Read the uploaded log file
    data = read_uploaded_file(uploaded_file)

    # Display a sample of the loaded data
    with st.expander("Sample of the loaded data"):
        st.dataframe(data.head().style.format({"Time": "{:.6}"}))

    # Filter by process name
    process_names = sorted(data["Process Name"].unique().tolist())
    process_names.insert(0, "ALL")
    filter_process = sidebar.selectbox(
        "Filter by Process Name:", options=process_names, key="process_names"
    )
    avail_sizes = sorted(
        np.unique((data["Size (B)"].unique() / toKB).astype(int)).tolist()
    )
    avail_sizes.pop(0)
    avail_sizes.insert(0, "ALL")
    filter_size = sidebar.selectbox(
        "Filter by Size (KB):", options=avail_sizes, key="avail_sizes"
    )

    sidebar.button("Clear Filters", on_click=reset_filters)

    # Rename disks if more than one disk is found
    disks_names = sorted(data["Disks"].unique().tolist())
    if len(disks_names) > 1:
        st.header(":pencil: Name your disks")
        st.write(
            "Your trace reports more than one disk in your system. You can rename your disks below."
        )
        new_names = {}
        for name in disks_names:
            new_names[name] = st.text_input(name + " name:", name)
        data = update_disks_names(data, new_names)

    # Check if the user selected a process name
    if filter_process != "ALL":
        data = data[data["Process Name"] == filter_process]

    # Check if the user selected a size to filter
    if filter_size != "ALL":
        data = data[data["Size (B)"] == filter_size * toKB]

    # Show log summary
    st.header(":page_facing_up: Data Summary")
    st.table(log_summary(data))

    # Plot summary
    plot_summary(data)

    st.header(":bar_chart: Request Size")
    with st.expander("Show info"):
        st.write(
            """
            The requested size is the amount of data requested by an application when reading/writing from/to a file.
            Typically, the requested size is a multiple of the sector size.

            The most important SSD performance metric for you will depend on your usage pattern.
            The charts below show the average request size and the number of requests for each size.
            This, along with the previously shown read/write and random/sequential ratios,
            can help you determine the most important aspect of an SSD disk to consider for your desired workload.
            """
        )
    show_request_size(data)

    # Show access time info
    st.header(":stopwatch: Performance")
    with st.expander("Show info"):
        st.write(
            """
            Access time refers to the duration (Disk Service Time) it requires to read or write a specific portion of a file.
            In other words, it is the time the disk spent to complete a request of a particular length (not considering the Windows I/O queue delay).

            I don't know why but access time reported by WPR is smaller than the access time measured by other benchmarking apps like CDM or AS SSD.
            """
        )
    show_performance(data, filter_size)


if __name__ == "__main__":
    # Increase pandas default output precision
    pd.set_option("display.precision", 7)
    pd.options.display.float_format = "{:.7f}".format

    # Set column names
    column_names = [
        "IO Type",
        "Priority",
        "Process Name",
        "Init Time (s)",
        "Complete Time (s)",
        "IO Time (µs)",
        "Disk Service Time (µs)",
        "Size (B)",
        "Min Offset",
        "Max Offset",
        "QD/I",
        "QD/C",
        "Disks",
        "Count",
    ]

    column_types = {
        "Count": int,
        "Disks": int,
        "QD/C": int,
        "QD/I": int,
        "Max Offset": str,
        "Min Offset": str,
        "Size (B)": str,
        "Disk Service Time (µs)": str,
        "IO Time (µs)": str,
        "Complete Time (s)": str,
        "Init Time (s)": str,
        "Process Name": str,
        "Priority": str,
        "IO Type": str,
    }

    # conversion factors from bytes
    toGB = 1024**3
    toMB = 1024**2
    toKB = 1024

    # Run the app
    t0 = timer()
    main()
    tf = timer()
    st.caption(f"Data processing took: {tf-t0:.2f} seconds.")
