"""Windows Disk Trace Visualizer.

A simple but useful web application designed to streamline the process of analyzing Event Trace Log (ETL) files,
specifically focusing on Disk I/O activities recorded by Windows Performance Recorder (or Xperf).

Author:   b g e n e t o @ g m a i l . c o m
History:  v1.0.0 Initial release
          v1.0.1 Updated profile link
          v1.0.2 Removed babel dependency and added locale_adjust_numbers function (faster)
                 Added custom profile download button
          v1.0.3 Split request size graphics into random and sequential categories.
                 Added option to remove disks.
          v1.0.4 Added Request Size by Duration chart.
          v1.0.5 Added IOPS chart and color consistency.
          v1.0.6 Better unit handling.
          v1.0.7 Improved label and number formatting. Added disk service time per process.
          v1.0.8 Added (all disks) Average Throughput and Average IOPS.
          v1.0.9 Better graph title.
          v1.0.10 Better graphics titles
          v1.0.11 Renamed graphics title
          v1.0.12 Fixed UnhashableParamError / LargeUtf8
          v1.0.13 Add Queue Depth, QoS, Misalignment, and Throughput over time analysis, and improve latency calculations with weighted quantiles.
          v1.0.14 Fix column name in avg. size and avg. service time charts.
Modified: 20260206
Usage:
    $ streamlit run windows-disk-trace-vis.py
"""

__VERSION__ = "1.0.13"

import base64
import urllib.error
import urllib.request
from timeit import default_timer as timer
from typing import Optional

import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st

try:
    pd.options.mode.string_storage = "python"
except (AttributeError, KeyError):
    pass

app_title = "Windows Disk Trace Visualizer"

about = """
Windows Performance Recorder ([WPR](https://learn.microsoft.com/en-us/windows-hardware/test/wpt/windows-performance-recorder))
is a performance recording tool that is based on Event Tracing for Windows ([ETW](https://learn.microsoft.com/en-us/windows/win32/etw/about-event-tracing)).
It records system and application events that you can analyze by parsing the generated Event Trace Log ([ETL](https://learn.microsoft.com/en-us/windows-hardware/drivers/devtest/trace-log)) file.
The ETL file format is a proprietary binary file format used by Microsoft Windows for collecting
and storing event traces generated by various components of the operating system. Since it is a poorly documented and
proprietary format, we need to use the [WPA Exporter](https://learn.microsoft.com/en-us/windows-hardware/test/wpt/exporter)
tool in order to convert the .etl file to a .csv file and then perform the desired trace analysis via csv file. Both WPR and WPA Exporter are part of the
Windows Assessment and Deployment Kit [Windows ADK](https://learn.microsoft.com/en-us/windows-hardware/get-started/adk-install).
"""

summary = f"""
"{app_title}" is a simple but useful web application designed to streamline the process of analyzing Event Trace Log (ETL) files,
specifically focusing on Disk I/O activities. This tool offers an intuitive and user-friendly interface to effortlessly navigate
through complex ETL data, providing insightful summaries and statistics for a thorough understanding of your disk performance requirements.
Key Features:
- **Data Analysis:** Uncover the most frequently requested data size, the percentage of random reading or writing, the average access time, and more.
- **Performance:** Identify the average throughput and IOPS for each disk, and compare the performance of sequential and random requests.
- **Filter Capability:** Filter data by process name or request size.

All this info is valuable for understanding where the performance of your disk/ssd matters most for your desired/traced workload/application.

Version {__VERSION__} made with [Streamlit](https://streamlit.io/) with ❤️ by [bgeneto](https://github.com/bgeneto/).
"""

python_svg = """
<svg width="800px" height="800px" viewBox="0 0 64 64" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M31.885 16c-8.124 0-7.617 3.523-7.617 3.523l.01 3.65h7.752v1.095H21.197S16 23.678 16 31.876c0 8.196 4.537 7.906 4.537 7.906h2.708v-3.804s-.146-4.537 4.465-4.537h7.688s4.32.07 4.32-4.175v-7.019S40.374 16 31.885 16zm-4.275 2.454c.771 0 1.395.624 1.395 1.395s-.624 1.395-1.395 1.395a1.393 1.393 0 0 1-1.395-1.395c0-.771.624-1.395 1.395-1.395z" fill="url(#a)"/><path d="M32.115 47.833c8.124 0 7.617-3.523 7.617-3.523l-.01-3.65H31.97v-1.095h10.832S48 40.155 48 31.958c0-8.197-4.537-7.906-4.537-7.906h-2.708v3.803s.146 4.537-4.465 4.537h-7.688s-4.32-.07-4.32 4.175v7.019s-.656 4.247 7.833 4.247zm4.275-2.454a1.393 1.393 0 0 1-1.395-1.395c0-.77.624-1.394 1.395-1.394s1.395.623 1.395 1.394c0 .772-.624 1.395-1.395 1.395z" fill="url(#b)"/><defs><linearGradient id="a" x1="19.075" y1="18.782" x2="34.898" y2="34.658" gradientUnits="userSpaceOnUse"><stop stop-color="#387EB8"/><stop offset="1" stop-color="#366994"/></linearGradient><linearGradient id="b" x1="28.809" y1="28.882" x2="45.803" y2="45.163" gradientUnits="userSpaceOnUse"><stop stop-color="#FFE052"/><stop offset="1" stop-color="#FFC331"/></linearGradient></defs></svg>
"""
pandas_svg = """
<svg data-name="Layer 1" version="1.1" viewBox="0 0 210.21 280.43" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<metadata><rdf:RDF><cc:Work rdf:about=""><dc:format>image/svg+xml</dc:format><dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage"/></cc:Work></rdf:RDF></metadata>
<defs><style>.cls-1{fill:#130754;}.cls-2{fill:#48e5ac;}.cls-3{fill:#e70488;}</style></defs>
<rect class="cls-1" x="74.51" y="43.03" width="24.09" height="50.02"/>
<rect class="cls-1" x="74.51" y="145.78" width="24.09" height="50.02"/>
<rect class="cls-2" x="74.51" y="107.65" width="24.09" height="23.6" fill="#ffca00"/>
<rect class="cls-1" x="35.81" y="84.15" width="24.09" height="166.27"/>
<rect class="cls-1" x="112.41" y="187.05" width="24.09" height="50.02"/>
<rect class="cls-1" x="112.41" y="84.21" width="24.09" height="50.02"/>
<rect class="cls-3" x="112.41" y="148.84" width="24.09" height="23.6"/>
<rect class="cls-1" x="150.3" y="30" width="24.09" height="166.27"/>
</svg>
"""

usage = """
[YouTube Quick Tutorial](https://youtu.be/XgtjpdunUoI)

First, you need to record a trace of your 'Disk I/O activity' with WPRUI.exe. You can trace your Windows boot process or a specific application/workload.
Then you need to convert the saved `.etl` file to a `.csv` file using the `wpaexporter.exe` tool that comes with [Windows ADK](https://learn.microsoft.com/en-us/windows-hardware/get-started/adk-install).
The `wpaexporter.exe` tool is typically located in the `C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit` folder. An example of usage follows:
```cmd
wpaexporter.exe -i boottrace.etl -profile DiskIO.wpaProfile -delimiter ;
```

If you want to trace your boot process do as follows:

- Open an elevated command prompt and run (change the path to your desired location):

   ```
   wpr -addboot GeneralProfile.Light -filemode -recordtempto D:\\Temp
   ```

After this, the trace will start automatically at the early stage of the next (re)boot.

- The command syntax to save the boot trace (.etl file) &mdash; after boot completion &mdash; is the following:

   ```
   wpr -stopboot D:\\Temp\\boottrace.etl
   ```

Now you can upload the (compressed) `.csv` file to this page. The page script &mdash; written in Python{python_svg}using Pandas{pandas_svg} &mdash; will analyze the single trace record using the provided
[profile](https://raw.githubusercontent.com/bgeneto/windows-disk-trace-vis/main/DiskIO.wpaProfile) and show you the results in tabular and graphical formats.
> **Note:** tested with WPR v10.0.25931.1000 on Windows 11 Pro version 22H2.
"""


def render_svg(svg, width="100%", height="100%") -> str:
    """Renders the given svg string."""
    b64 = base64.b64encode(svg.encode("utf-8")).decode("utf-8")
    html = (
        rf'<img width={width} height={height} src="data:image/svg+xml;base64,{b64}"/>'
    )
    return html


def weighted_quantile(
    df: pd.DataFrame, val_col: str, weight_col: str, q: float
) -> float:
    """Calculate weighted quantile."""
    df_sorted = df.sort_values(val_col)
    cumsum = df_sorted[weight_col].cumsum()
    cutoff = df_sorted[weight_col].sum() * q
    return df_sorted[cumsum >= cutoff][val_col].iloc[0]


def locale_adjust_numbers(data: pd.DataFrame) -> pd.DataFrame:
    """Check number format and adjust data accordingly."""
    int_cols = ["Size (B)"]
    float_cols = [
        "Init Time (s)",
        "Complete Time (s)",
        "Disk Service Time (µs)",
        "IO Time (µs)",
    ]

    # remove spaces, dots and commas from integer columns
    for col in int_cols:
        data[col] = (
            data[col].str.replace(" ", "").str.replace(".", "").str.replace(",", "")
        ).astype(int)

    # locale aware format float columns
    if data["Init Time (s)"][:10].str.contains(",").all():
        for col in float_cols:
            data[col] = (
                data[col]
                .str.replace(" ", "")
                .str.replace(".", "")
                .str.replace(",", ".")
                .astype(float)
            )
    else:
        for col in float_cols:
            data[col] = (
                data[col].str.replace(" ", "").str.replace(",", "").astype(float)
            )

    return data


def ensure_python_string_columns(
    data: pd.DataFrame, columns: list[str]
) -> pd.DataFrame:
    """Force Python-backed string storage for specific columns to avoid Arrow LargeUtf8."""
    for col in columns:
        if col in data.columns:
            try:
                data[col] = data[col].astype("string[python]")
            except TypeError:
                data[col] = data[col].astype(str)
    return data


@st.cache_data
def read_uploaded_file(uploaded_file) -> pd.DataFrame:
    """Read the uploaded file using pandas."""

    # check if the uploaded file is compressed
    compression = None
    if uploaded_file.name.endswith(".zip"):
        compression = "zip"
    elif uploaded_file.name.endswith(".gz"):
        compression = "gzip"
    elif uploaded_file.name.endswith(".bz2"):
        compression = "bz2"

    # Read the uploaded file using pandas
    data = pd.read_csv(
        uploaded_file,
        sep=";",
        header=None,
        names=column_names,
        dtype=column_types,
        compression=compression,
        skiprows=1,
    )

    data = ensure_python_string_columns(
        data,
        ["IO Type", "Priority", "Process Name", "Min Offset", "Max Offset"],
    )

    # Drop rows where "IO Type" is not "Read" or "Write"
    data = data[data["IO Type"].isin(["Read", "Write"])]

    # Drop rows where "Priority" is not "Normal"
    data = data[data["Priority"] == "Normal"]

    data = locale_adjust_numbers(data)

    # Convert "Disks" to string so plotly can recognize it as categorical (not numerical, continuous)
    data["Disks"] = "Disk " + data["Disks"].astype(str)

    # Convert "Min Offset" and "Max Offset" from hex to int (vectorized for performance)
    data["Min Offset"] = data["Min Offset"].map(lambda x: int(x, 16))
    data["Max Offset"] = data["Max Offset"].map(lambda x: int(x, 16))

    # Order by disk and time to correctly classify SEQ/RND per disk
    data = data.sort_values(by=["Disks", "Init Time (s)"], ignore_index=True)

    # Classify the operation type as "SEQ" or "RND" per disk
    # Compare the "Max Offset" from previous row with the "Min Offset" from current row
    # Use a spatial locality threshold (1MB = 1048576 bytes)
    # Also enforce same IO type and a small time gap to reduce false positives
    def classify_seq_rnd(group: pd.DataFrame) -> pd.Series:
        group = group.sort_values(by="Init Time (s)")
        prev_max = group["Max Offset"].shift(1)
        prev_time = group["Init Time (s)"].shift(1)
        prev_io = group["IO Type"].shift(1)
        diff = group["Min Offset"] - prev_max
        time_gap = (group["Init Time (s)"] - prev_time).abs()
        threshold = 1048576
        time_threshold = 0.01  # 10 ms
        is_seq = (
            (prev_io == group["IO Type"])
            & (diff >= 0)
            & (diff <= threshold)
            & (time_gap <= time_threshold)
        )
        return pd.Series(np.where(is_seq, "SEQ", "RND"), index=group.index)

    data["Category"] = data.groupby(["Disks"], group_keys=False).apply(
        lambda g: pd.Series(classify_seq_rnd(g), index=g.index)
    )

    return data.dropna(how="any")


@st.cache_data
def update_disks_names(data: pd.DataFrame, names: dict) -> pd.DataFrame:
    """Change disk name."""
    for old_name, new_name in names.items():
        data.loc[data["Disks"] == old_name, "Disks"] = new_name

    return data


def log_summary(data: pd.DataFrame) -> pd.DataFrame:
    """Show totals."""

    summary = {}

    # Compute the total monitoring time using wall-clock duration
    start_time = data["Init Time (s)"].min()
    end_time = data["Complete Time (s)"].max()
    monitoring_time_sec = max(end_time - start_time, 0)
    summary["Monitoring time"] = "{:.2f} seconds".format(monitoring_time_sec)

    # Weighted average time for each operation
    total_ios = data["Count"].sum()
    total_disk_time_weighted = (data["Count"] * data["Disk Service Time (µs)"]).sum()
    summary["Average access time"] = "{:.2f} µs".format(
        total_disk_time_weighted / total_ios
    )

    # Weighted Latency percentiles (P50 and P99)
    summary["P50 access time (median)"] = "{:.2f} µs".format(
        weighted_quantile(data, "Disk Service Time (µs)", "Count", 0.50)
    )
    summary["P99 access time"] = "{:.2f} µs".format(
        weighted_quantile(data, "Disk Service Time (µs)", "Count", 0.99)
    )

    # Calculate weighted totals using Count column for correctness
    # Each row may represent multiple I/O operations (Count >= 1)
    # Disk service time is per-request, so weight by Count for total busy time
    total_disk_time_sec = (data["Count"] * data["Disk Service Time (µs)"]).sum() * 1e-6
    total_bytes = (data["Count"] * data["Size (B)"]).sum()  # Weighted by Count

    # Calculate observed throughput (wall-clock time)
    observed_throughput = (
        (total_bytes / toMB) / monitoring_time_sec if monitoring_time_sec > 0 else 0
    )
    summary["Observed throughput"] = "{:.2f} MB/s".format(observed_throughput)

    # Calculate observed IOPS (wall-clock time)
    observed_iops = total_ios / monitoring_time_sec if monitoring_time_sec > 0 else 0
    summary["Observed IOPS"] = "{:.2f}".format(observed_iops)

    # number of read and write requests (weighted by Count)
    summary["Read requests"] = int(data[data["IO Type"] == "Read"]["Count"].sum())
    summary["Write requests"] = int(data[data["IO Type"] == "Write"]["Count"].sum())

    # total requests
    summary["Total requests"] = summary["Read requests"] + summary["Write requests"]

    # percentage of read and write requests
    summary["Percent READ"] = "{:.2f}%".format(
        summary["Read requests"] / summary["Total requests"] * 100
    )
    summary["Percent WRITE"] = "{:.2f}%".format(
        summary["Write requests"] / summary["Total requests"] * 100
    )

    # total percentage SEQ and RND requests (weighted by Count)
    summary["Percent RANDOM"] = "{:.2f}%".format(
        data[data["Category"] == "RND"]["Count"].sum() / summary["Total requests"] * 100
    )
    summary["Percent SEQUENTIAL"] = "{:.2f}%".format(
        data[data["Category"] == "SEQ"]["Count"].sum() / summary["Total requests"] * 100
    )

    # total data read in GBytes
    summary["Read data size"] = "{:.2f} GB".format(
        ((data["Count"] * data["Size (B)"]).where(data["IO Type"] == "Read")).sum()
        / toGB
    )

    # total data written in GBytes
    summary["Write data size"] = "{:.2f} GB".format(
        (data["Count"] * data[data["IO Type"] == "Write"]["Size (B)"]).sum() / toGB
    )

    # total data size in GBytes
    summary["Total data size"] = "{:.2f} GB".format(
        (data["Count"] * data["Size (B)"]).sum() / toGB
    )

    read_data = data[data["IO Type"] == "Read"]
    write_data = data[data["IO Type"] == "Write"]

    # min and max read requests in KB
    summary["Min. read request size"] = "{:.1f} KB".format(
        read_data["Size (B)"].min() / toKB
    )

    # avg read request in KB (weighted by Count)
    summary["Avg. read request size"] = "{:.1f} KB".format(
        (read_data["Size (B)"] * read_data["Count"]).sum()
        / read_data["Count"].sum()
        / toKB
    )

    # max read requests in KB
    number = read_data["Size (B)"].max() / toKB
    formatted_number = (
        "{:.1f}".format(number) if number % 1 else "{:.0f}".format(number)
    )
    summary["Max. read request size"] = formatted_number + " KB"

    # min and max write requests in KB
    summary["Min. write request size"] = "{:.1f} KB".format(
        write_data["Size (B)"].min() / toKB
    )

    # avg write request in KB (weighted by Count)
    summary["Avg. write request size"] = "{:.1f} KB".format(
        (write_data["Size (B)"] * write_data["Count"]).sum()
        / write_data["Count"].sum()
        / toKB
    )

    # max write requests in KB
    number = write_data["Size (B)"].max() / toKB
    formatted_number = (
        "{:.1f}".format(number) if number % 1 else "{:.0f}".format(number)
    )
    summary["Max. write request size"] = formatted_number + " KB"

    return pd.DataFrame.from_dict(summary, orient="index", columns=["Value"])


def find_proper_unit(data: pd.DataFrame, col: str) -> tuple:
    """Find the proper unit for the given dataframe column."""
    unit = "B"
    factor = 1
    avg = data[col].mean()
    if avg / toGB > 1:
        factor = toGB
        unit = "GB"
    elif avg / toMB > 1:
        factor = toMB
        unit = "MB"
    elif avg / toKB > 1:
        factor = toKB
        unit = "KB"
    return (factor, unit)


def plot_summary(data: pd.DataFrame):
    # Total requests in GB

    df = (data.groupby(["Disks", "IO Type"])["Size (B)"].sum()).to_frame().reset_index()

    # Find the proper unit for the size
    factor, unit = find_proper_unit(df, "Size (B)")
    df["Size (B)"] = df["Size (B)"] / factor
    df.rename(columns={"Size (B)": "Size"}, inplace=True)

    # Calculate percentage within each disk group
    df["Percent"] = df["Size"] / df.groupby("Disks")["Size"].transform("sum") * 100

    # Create a plotly bar chart
    fig = px.bar(
        df,
        x="Disks",
        y=f"Size",
        title="Requested Data Size",
        color="IO Type",
        color_discrete_map=io_type_color_mapping,
        barmode="group",
        text="Percent",
    )

    # Annotate the bars with percentage values
    fig.update_traces(texttemplate="%{text:.3s}%", textposition="inside")
    fig.update_layout(yaxis_title=f"Size ({unit})")

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)
        st.write(
            '> **Note:** There is a difference between the percentages shown on this chart and those that are represented as "Percent READ" and "Percent WRITE" in the summary table above.'
            + " The difference between the two is that the chart shows the percentage of data size, whilst the table shows the percentage of (the number of) requests. "
            + " Numerous (short length) queries may result in a higher number of requests but a lesser amount of data being read or written since the size (in Bytes) of each request can vary."
        )

    # RND and SEQ requested data size in GB
    # --------------------------------------
    df = (
        (data.groupby(["Disks", "Category"])["Size (B)"].sum()).to_frame().reset_index()
    )

    # Find the proper unit for the size
    factor, unit = find_proper_unit(df, "Size (B)")
    df["Size (B)"] = df["Size (B)"] / factor
    df.rename(columns={"Size (B)": "Size"}, inplace=True)

    # Calculate percentage within each disk group
    df["Percent"] = df["Size"] / df.groupby("Disks")["Size"].transform("sum") * 100

    # Create a plotly bar chart
    fig = px.bar(
        df,
        x="Disks",
        y="Size",
        title="Requested Data Size by Category (Random/Sequential)",
        color="Category",
        color_discrete_map=category_color_mapping,
        barmode="group",
        text="Percent",
    )

    # Annotate the bars with percentage values
    fig.update_traces(texttemplate="%{text:.3s}%", textposition="inside")
    fig.update_layout(yaxis_title=f"Size ({unit})")

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)
        st.write(
            '> **Note:** The percentage displayed on this chart is distinct from the "Percent RANDOM" and "Percent SEQUENTIAL" indicated in '
            + "the Data Summary table above. This distinction arises because the table presents the percentage of RND/SEQ requests, whereas the chart illustrates "
            + "the percentage based on data size."
        )

    # RND and SEQ requested data by request type per disk
    # --------------------------------------
    disks_names = sorted(data["Disks"].unique().tolist())
    for disk_name in disks_names:
        df = (
            (
                data[data["Disks"] == disk_name]
                .groupby(["IO Type", "Category"])["Size (B)"]
                .sum()
            )
            .to_frame()
            .reset_index()
        )

        # Find the proper unit for the size
        factor, unit = find_proper_unit(df, "Size (B)")
        df["Size (B)"] = df["Size (B)"] / factor
        df.rename(columns={"Size (B)": "Size"}, inplace=True)

        # Calculate percentage within each disk group
        df["Percent"] = (
            df["Size"] / df.groupby("Category")["Size"].transform("sum") * 100
        )

        # Create a plotly bar chart
        fig = px.bar(
            df,
            x="Category",
            y="Size",
            title=f"Requested Data Size by Category (RND/SEQ) and IO Type (R/W) ({disk_name})",
            color="IO Type",
            color_discrete_map=io_type_color_mapping,
            barmode="group",
            text="Percent",
        )

        # Annotate the bars with percentage values
        fig.update_traces(texttemplate="%{text:.3s}%", textposition="inside")
        fig.update_layout(yaxis_title=f"Size ({unit})")

        st.plotly_chart(fig, use_container_width=True)

        with st.expander("Show data"):
            st.dataframe(df)


def outlier_thresholds_iqr(data: pd.DataFrame, col_name: str, lth=0.05, uth=0.95):
    """
    Calculates the lower and upper outlier thresholds using the interquartile range (IQR) method.

    Args:
        data (pd.DataFrame): The input DataFrame.
        col_name (str): The column name from which to calculate the thresholds.
        th1 (float): The lower percentile for the quartile calculation. Defaults to 0.05.
        th3 (float): The upper percentile for the quartile calculation. Defaults to 0.95.

    Returns:
        float: The lower outlier threshold.
        float: The upper outlier threshold.
    """
    quartile1 = weighted_quantile(data, col_name, "Count", lth)
    quartile3 = weighted_quantile(data, col_name, "Count", uth)
    iqr = quartile3 - quartile1
    upper_limit = quartile3 + 1.5 * iqr
    lower_limit = quartile1 - 1.5 * iqr
    return lower_limit, upper_limit


def show_performance(
    data: pd.DataFrame,
    filter_size,
    latency_col: str,
    remove_outliers: bool = False,
):
    """Compute disk access time and other performance metrics."""
    # Remove outliers
    fdf = data
    if remove_outliers:
        _, ub = outlier_thresholds_iqr(data, latency_col, 0.01, 0.99)

        # Filter the DataFrame based on quartiles
        fdf = data[(data[latency_col] > 0) & (data[latency_col] <= ub)]
        removed_pct = (1 - (fdf["Count"].sum() / data["Count"].sum())) * 100
        st.caption(f"Outlier filter removed {removed_pct:.2f}% of I/Os.")

    # average access time by disk
    df = fdf.groupby(["Disks"]).apply(
        lambda x: (x[latency_col] * x["Count"]).sum() / x["Count"].sum()
    )
    df.name = latency_col

    # Create Plotly bar plot
    fig = px.bar(
        df.reset_index(),
        x="Disks",
        y=latency_col,
        title=f"Average Access Time [{latency_col}]",
        color="Disks",
        labels={latency_col: f"Average Access Time [{latency_col}]"},
    )

    # Set x-axis tickmode to "array" and provide the index values to only show those
    fig.update_xaxes(tickmode="array", tickvals=df.index)

    # Add unit to hover text using hovertemplate
    fig.update_traces(
        hovertemplate="Disk: %{x}<br>Average Time: %{y:.2f} µs<extra></extra>"
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average by disk and by request type
    df = fdf.groupby(["Disks", "IO Type"]).apply(
        lambda x: (x[latency_col] * x["Count"]).sum() / x["Count"].sum()
    )
    df.name = latency_col

    # Create an interactive grouped bar plot using Plotly
    fig = px.bar(
        df.unstack(),
        color="IO Type",
        color_discrete_map=io_type_color_mapping,
        barmode="group",
        title=f"Average Access Time per IO Type [{latency_col}]",
        labels={"value": f"Average Access Time [{latency_col}]"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average throughput by disk (weighted by Count for correctness)
    total_time_sec = (
        fdf.groupby(["Disks"]).apply(
            lambda g: (g["Count"] * g["Disk Service Time (µs)"]).sum()
        )
    ) * 1e-6
    total_bytes = fdf.groupby(["Disks"]).apply(
        lambda g: (g["Count"] * g["Size (B)"]).sum()
    )
    df = (total_bytes / toMB) / total_time_sec
    df.rename("Service-rate Throughput (MB/s)", inplace=True)

    fig = px.bar(
        df.reset_index(),
        x="Disks",
        y="Service-rate Throughput (MB/s)",
        color="Disks",
        title="Service-rate Throughput",
        labels={"value": "Service-rate Throughput (MB/s)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # observed throughput by disk (wall-clock time)
    duration_sec = fdf.groupby(["Disks"]).apply(
        lambda g: g["Complete Time (s)"].max() - g["Init Time (s)"].min()
    )
    duration_sec = duration_sec.where(duration_sec > 0, np.nan)
    df = (total_bytes / toMB) / duration_sec
    df = df.fillna(0)
    df.rename("Observed Throughput (MB/s)", inplace=True)

    fig = px.bar(
        df.reset_index(),
        x="Disks",
        y="Observed Throughput (MB/s)",
        color="Disks",
        title="Observed Throughput (Wall-clock)",
        labels={"value": "Observed Throughput (MB/s)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # service-rate throughput by disk and by IO type (weighted by Count)
    total_time_sec = (
        fdf.groupby(["Disks", "IO Type"]).apply(
            lambda g: (g["Count"] * g["Disk Service Time (µs)"]).sum()
        )
    ) * 1e-6
    total_bytes = fdf.groupby(["Disks", "IO Type"]).apply(
        lambda g: (g["Count"] * g["Size (B)"]).sum()
    )
    df = (total_bytes / toMB) / total_time_sec

    # Create an interactive grouped bar plot using Plotly
    fig = px.bar(
        df.unstack(),
        color="IO Type",
        color_discrete_map=io_type_color_mapping,
        barmode="group",
        title="Service-rate Throughput per IO Type",
        labels={"value": "Service-rate Throughput (MB/s)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # observed throughput by disk and IO type (wall-clock time)
    duration_sec = fdf.groupby(["Disks", "IO Type"]).apply(
        lambda g: g["Complete Time (s)"].max() - g["Init Time (s)"].min()
    )
    duration_sec = duration_sec.where(duration_sec > 0, np.nan)
    df = (total_bytes / toMB) / duration_sec
    df = df.fillna(0)

    fig = px.bar(
        df.unstack(),
        color="IO Type",
        color_discrete_map=io_type_color_mapping,
        barmode="group",
        title="Observed Throughput per IO Type (Wall-clock)",
        labels={"value": "Observed Throughput (MB/s)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # service-rate throughput per category (weighted by Count)
    disks_names = sorted(data["Disks"].unique().tolist())
    for disk_name in disks_names:
        disk_data = fdf[fdf["Disks"] == disk_name]
        total_time_sec = (
            disk_data.groupby(["IO Type", "Category"]).apply(
                lambda g: (g["Count"] * g["Disk Service Time (µs)"]).sum()
            )
        ) * 1e-6

        total_bytes = disk_data.groupby(["IO Type", "Category"]).apply(
            lambda g: (g["Count"] * g["Size (B)"]).sum()
        )

        df = (total_bytes / toMB) / total_time_sec
        df.rename("Service-rate Throughput (MB/s)", inplace=True)

        # Create an interactive grouped bar plot using Plotly
        fig = px.bar(
            df.reset_index(),
            x="Category",
            y="Service-rate Throughput (MB/s)",
            color="IO Type",
            color_discrete_map=io_type_color_mapping,
            barmode="group",
            title=f"Service-rate Throughput per IO Type and Category ({disk_name})",
            labels={"value": "Service-rate Throughput (MB/s)"},
        )

        st.plotly_chart(fig, use_container_width=True)

        with st.expander("Show data"):
            st.dataframe(df)

    # service-rate IOPS (weighted by Count)
    total_time_sec = (
        fdf.groupby(["Disks", "IO Type"]).apply(
            lambda g: (g["Count"] * g["Disk Service Time (µs)"]).sum()
        )
    ) * 1e-6
    total_ios = fdf.groupby(["Disks", "IO Type"])["Count"].sum()
    df = total_ios / total_time_sec

    # Create an interactive grouped bar plot using Plotly
    fig = px.bar(
        df.unstack(),
        color="IO Type",
        color_discrete_map=io_type_color_mapping,
        barmode="group",
        title=f"Service-rate IOPS (size: {filter_size} KB)",
        labels={"value": "Service-rate IOPS"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # observed IOPS by disk and IO type (wall-clock time)
    duration_sec = fdf.groupby(["Disks", "IO Type"]).apply(
        lambda g: g["Complete Time (s)"].max() - g["Init Time (s)"].min()
    )
    duration_sec = duration_sec.where(duration_sec > 0, np.nan)
    total_ios = fdf.groupby(["Disks", "IO Type"])["Count"].sum()
    df = total_ios / duration_sec
    df = df.fillna(0)

    fig = px.bar(
        df.unstack(),
        color="IO Type",
        color_discrete_map=io_type_color_mapping,
        barmode="group",
        title=f"Observed IOPS (Wall-clock) (size: {filter_size} KB)",
        labels={"value": "Observed IOPS"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)


def custom_int_float_format(value):
    return "{:.1f}".format(value) if value % 1 else "{:.0f}".format(value)


def show_request_size_histogram(data: pd.DataFrame):
    """Show request size distribution as a histogram."""
    for disk_name in sorted(data["Disks"].unique().tolist()):
        df = data[data["Disks"] == disk_name].copy()

        # Convert size to KB for better readability
        df["Size (KB)"] = df["Size (B)"] / toKB

        # Create histogram with separate traces for Read/Write
        fig = px.histogram(
            df,
            x="Size (KB)",
            color="IO Type",
            color_discrete_map=io_type_color_mapping,
            barmode="overlay",
            nbins=50,
            title=f"Request Size Distribution ({disk_name})",
            labels={"Size (KB)": "Request Size (KB)", "count": "Frequency"},
            opacity=0.7,
            weights="Count",
        )

        fig.update_layout(
            xaxis_title="Request Size (KB)",
            yaxis_title="Frequency (weighted requests)",
            bargap=0.1,
        )

        # Add logarithmic scale option for better visualization of skewed data
        fig.update_xaxes(type="log")

        st.plotly_chart(fig, use_container_width=True)

        with st.expander("Show statistics"):

            def weighted_stats(g: pd.DataFrame) -> pd.Series:
                weights = g["Count"]
                sizes = g["Size (KB)"]
                mean = (sizes * weights).sum() / weights.sum()
                median = weighted_quantile(g, "Size (KB)", "Count", 0.5)
                variance = np.average((sizes - mean) ** 2, weights=weights)
                std_dev = np.sqrt(variance)
                return pd.Series(
                    {
                        "Min": sizes.min(),
                        "Max": sizes.max(),
                        "Mean": mean,
                        "Median": median,
                        "Std Dev": std_dev,
                    }
                )

            stats_df = df.groupby("IO Type").apply(weighted_stats).round(2)
            st.dataframe(stats_df)
            st.write(
                "> **Note:** The histogram uses a logarithmic x-axis scale to better visualize "
                "the distribution of request sizes, which typically spans several orders of magnitude."
            )


def show_request_size_count(data: pd.DataFrame):
    """Show most frequent request size per total request count."""
    # compute average read and write request size in kbytes
    df = (
        (
            data.groupby(["Disks", "IO Type"]).apply(
                lambda g: (g["Size (B)"] * g["Count"]).sum() / g["Count"].sum() / toKB
            )
        )
        .to_frame()
        .reset_index()
    )

    # rename the columns (groupby apply returns column as 0, not "Size (B)")
    df.rename(columns={0: "Avg. Size (KB)"}, inplace=True)

    # Create Plotly bar plot
    fig = px.bar(
        df,
        x="Disks",
        y="Avg. Size (KB)",
        color="IO Type",
        color_discrete_map=io_type_color_mapping,
        barmode="group",
        title="Average Request Size",
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # Count occurrences of specific "Size (B)"
    length_counts_df = (
        data.groupby(["Disks", "IO Type", "Size (B)"])["Count"]
        .sum()
        .sort_index()
        .to_frame()
        .reset_index()
    )

    for disk_name in sorted(length_counts_df["Disks"].unique().tolist()):
        df = length_counts_df[length_counts_df["Disks"] == disk_name].copy()
        df["Request Size"] = (
            (df["Size (B)"] / toKB)
            .apply(lambda x: "{:.1f}KB".format(x) if x % 1 else "{:.0f}KB".format(x))
            .astype(str)
        )
        df.drop(["Disks", "Size (B)"], axis=1, inplace=True)
        # Calculate percentage within each request group
        df["Percent"] = (
            df["Count"] / df.groupby("IO Type")["Count"].transform("sum") * 100
        )
        # Identify rows to drop
        rows_to_drop = df.index[df["Percent"] < percentage_threshold]
        # Drop rows
        df.drop(index=rows_to_drop, inplace=True)
        # Create the plot
        fig = px.bar(
            df.sort_values(by=["Percent"], ascending=False),
            x="Request Size",
            y="Percent",
            color="IO Type",
            color_discrete_map=io_type_color_mapping,
            barmode="group",
            title=f"Number of Requests Percent per Request Size ({disk_name})",
            text="Percent",
            custom_data=["IO Type", "Percent", "Count"],
        )
        # Annotate the bars with percentage values
        fig.update_xaxes(type="category")
        fig.update_traces(
            texttemplate="%{text:.3s}%",
            textposition="inside",
            hovertemplate="Size: %{x}<br>Type: %{customdata[0]}<br>Percent: %{customdata[1]:.1f}%<br>Count: %{customdata[2]}<extra></extra>",
        )
        fig.update_layout(
            xaxis={"categoryorder": "total descending"},
            yaxis_title="Percent of Requests",
        )
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("Show data"):
            st.dataframe(df)


def show_request_size_bytes(data: pd.DataFrame):
    """Show most frequent request size per total bytes."""

    # Count occurrences of specific "Size (B)"
    length_bytes_df = (
        data.groupby(["Disks", "IO Type", "Size (B)"])["Count"]
        .sum()
        .sort_index()
        .to_frame()
        .reset_index()
    )
    length_bytes_df["Total Size"] = (
        length_bytes_df["Size (B)"] * length_bytes_df["Count"]
    )

    # find the best unit to display the total size by averaging the total size
    # and then dividing by the best unit
    avg = length_bytes_df["Total Size"].mean()
    if avg / toGB > 1:
        length_bytes_df["Total Size"] = length_bytes_df["Total Size"] / toGB
        unit = "GB"
    elif avg / toMB > 1:
        length_bytes_df["Total Size"] = length_bytes_df["Total Size"] / toMB
        unit = "MB"
    elif avg / toKB > 1:
        length_bytes_df["Total Size"] = length_bytes_df["Total Size"] / toKB
        unit = "KB"
    else:
        length_bytes_df["Total Size"] = length_bytes_df["Total Size"]
        unit = "B"

    for disk_name in sorted(length_bytes_df["Disks"].unique().tolist()):
        df = length_bytes_df[length_bytes_df["Disks"] == disk_name].copy()
        df["Request Size"] = (
            (df["Size (B)"] / toKB)
            .apply(lambda x: "{:.1f}KB".format(x) if x % 1 else "{:.0f}KB".format(x))
            .astype(str)
        )
        # Calculate percentage within each request group
        df["Percent"] = (
            df["Total Size"]
            / df.groupby("IO Type")["Total Size"].transform("sum")
            * 100
        )
        # drop unused columns
        df.drop(["Disks", "Size (B)", "Count"], axis=1, inplace=True)
        # Identify rows to drop
        rows_to_drop = df.index[df["Percent"] < percentage_threshold]
        # Drop rows
        df.drop(index=rows_to_drop, inplace=True)
        # Create the plot
        fig = px.bar(
            df.sort_values(by=["Percent"], ascending=False),
            x="Request Size",
            y="Percent",
            color="IO Type",
            color_discrete_map=io_type_color_mapping,
            barmode="group",
            title=f"Data Requested per Request Size ({disk_name})",
            text="Percent",
            custom_data=["IO Type", "Percent", "Total Size"],
        )
        # Annotate the bars with percentage values
        fig.update_xaxes(type="category")
        fig.update_traces(
            texttemplate="%{text:.3s}%",
            textposition="inside",
            hovertemplate="Size: %{x}<br>Type: %{customdata[0]}<br>Percent: %{customdata[1]:.1f}%<br>Data: %{customdata[2]:.2f}"
            + unit
            + "<extra></extra>",
        )
        fig.update_layout(
            xaxis={"categoryorder": "total descending"},
            yaxis_title="Percent of Requested Data",
        )
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("Show data"):
            st.dataframe(df)
            st.write(
                f"> **Note:** Percentages below the threshold ({percentage_threshold}%) are not displayed. "
                + "The slider in the :point_left: left sidebar allows you to modify this threshold value. "
                + "Decrease the threshold to show more data, or increase it to show less data."
            )


def show_request_size_iops(data: pd.DataFrame):
    """Compute IOPS per request size."""
    for disk_name in sorted(data["Disks"].unique().tolist()):
        df = data[data["Disks"] == disk_name].copy()
        df = (
            df.groupby(["IO Type", "Size (B)"])
            .apply(
                lambda g: pd.Series(
                    {
                        "total_time": (g["Disk Service Time (µs)"] * g["Count"]).sum(),
                        "request_count": g["Count"].sum(),
                    }
                )
            )
            .reset_index()
        )
        # compute iops
        df["iops"] = df["request_count"] / (df["total_time"] / 1e6)

        # request size with proper unit
        df["Request Size"] = (
            (df["Size (B)"] / toKB)
            .apply(lambda x: "{:.1f}KB".format(x) if x % 1 else "{:.0f}KB".format(x))
            .astype(str)
        )

        # show only the top requests
        df = (
            df.sort_values(
                by=["request_count"], ascending=False
            )  # do not sort by iops here
            .groupby("IO Type")
            .head(top_charts_count)
            .reset_index(drop=True)
        )

        # Create a plotly bar chart
        fig = px.bar(
            df.sort_values(by=["iops"], ascending=False),
            x="Request Size",
            y="iops",
            title=f"Top {top_charts_count} IOPS per Request Size ({disk_name})",
            color="IO Type",
            color_discrete_map=io_type_color_mapping,
            barmode="group",
        )
        fig.update_xaxes(type="category")
        fig.update_yaxes(tickformat=".2s")
        fig.update_layout(
            xaxis={"categoryorder": "total descending"},
            yaxis_title="IOPS",
        )
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("Show data"):
            st.dataframe(df)


def show_request_size_time(data: pd.DataFrame):
    """Compute total disk service time per request size."""

    # One chart for each disk
    for disk_name in sorted(data["Disks"].unique().tolist()):
        df = data[data["Disks"] == disk_name].copy()
        df = (
            df.groupby(["IO Type", "Category", "Size (B)"])
            .apply(lambda g: (g["Disk Service Time (µs)"] * g["Count"]).sum())
            .to_frame()
            .reset_index()
        )

        # rename column (groupby apply returns column as 0)
        df.rename(columns={0: "Disk Service Time"}, inplace=True)

        # request size with proper unit
        df["Request Size"] = (
            (df["Size (B)"] / toKB)
            .apply(lambda x: "{:.1f}KB".format(x) if x % 1 else "{:.0f}KB".format(x))
            .astype(str)
        )

        # Calculate percentage within each disk group
        df["Percent"] = (
            df["Disk Service Time"]
            / df.groupby("IO Type")["Disk Service Time"].transform("sum")
            * 100
        )

        # find proper unit for disk service time
        avg = df["Disk Service Time"].mean()
        if avg / toSec > 1:
            df["Disk Service Time"] = df["Disk Service Time"] / toSec
            unit = "s"
        elif avg / toMSec > 1:
            df["Disk Service Time"] = df["Disk Service Time"] / toMSec
            unit = "ms"
        else:
            unit = "µs"

        df.drop(["Size (B)"], axis=1, inplace=True)

        # Drop rows
        rows_to_drop = df.index[df["Percent"] < percentage_threshold]
        df.drop(index=rows_to_drop, inplace=True)

        # Create a plotly bar chart
        fig = px.bar(
            df.sort_values(by=["Percent"], ascending=False),
            x="Request Size",
            y="Percent",
            title=f"Disk Service Time per Request Size ({disk_name})",
            color="IO Type",
            color_discrete_map=io_type_color_mapping,
            barmode="group",
            text="Percent",
            custom_data=["IO Type", "Percent", "Disk Service Time", "Category"],
        )

        # Annotate the bars with percentage values
        fig.update_xaxes(type="category")
        fig.update_traces(
            texttemplate="%{text:.3s}%",
            textposition="inside",
            hovertemplate="Size: %{x}<br>Type: %{customdata[0]}<br>Percent: %{customdata[1]:.1f}%<br>Time: %{customdata[2]:.2f}"
            + unit
            + "<br>Category: %{customdata[3]}<extra></extra>",
        )
        fig.update_layout(
            xaxis={"categoryorder": "total descending"},
            yaxis_title="Percent of Disk Service Time",
        )
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("Show data"):
            st.dataframe(df)


def show_request_size_process(data: pd.DataFrame):
    """Compute request data size per process."""
    for disk_name in sorted(data["Disks"].unique().tolist()):
        df = data[data["Disks"] == disk_name].copy()
        df["Total Size"] = df["Count"] * df["Size (B)"]

        df = (df.groupby(["Process Name", "IO Type"])["Total Size"].sum()).to_frame()

        # show only the top 10 processes
        df = (
            df.sort_values(by=["Total Size"], ascending=False)
            .head(top_charts_count)
            .reset_index()
        )

        # find the best unit to display the total size by averaging the total size
        # and then dividing by the best unit
        avg = df["Total Size"].mean()
        if avg / toGB > 1:
            df["Total Size"] = df["Total Size"] / toGB
            unit = "GB"
        elif avg / toMB > 1:
            df["Total Size"] = df["Total Size"] / toMB
            unit = "MB"
        elif avg / toKB > 1:
            df["Total Size"] = df["Total Size"] / toKB
            unit = "KB"
        else:
            unit = "B"

        fig = px.bar(
            df,
            x="Process Name",
            y="Total Size",
            title=f"Top {top_charts_count} Requested Data Size per Process ({disk_name})",
            color="IO Type",
            color_discrete_map=io_type_color_mapping,
            barmode="group",
        )
        fig.update_xaxes(type="category")
        fig.update_layout(
            xaxis={"categoryorder": "total descending"},
            yaxis_title=f"Requested Data Size ({unit})",
        )
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("Show data"):
            st.dataframe(df)


def show_service_time_process(data: pd.DataFrame):
    """Compute total disk service time per process."""
    for disk_name in sorted(data["Disks"].unique().tolist()):
        df = data[data["Disks"] == disk_name].copy()
        df = (
            df.groupby(["Process Name", "IO Type"]).apply(
                lambda g: (g["Disk Service Time (µs)"] * g["Count"]).sum()
            )
        ).to_frame()

        # show only the top 10 processes (groupby apply returns column as 0)
        df = (
            df.sort_values(by=[0], ascending=False).head(top_charts_count).reset_index()
        )

        # find the best unit to display the total size by averaging
        avg = df[0].mean()
        if avg / toSec > 1:
            df["Disk Service Time"] = df[0] / toSec
            unit = "s"
        elif avg / toMSec > 1:
            df["Disk Service Time"] = df[0] / toMSec
            unit = "ms"
        else:
            df["Disk Service Time"] = df[0]
            unit = "µs"

        # drop unused columns
        df.drop([0], axis=1, inplace=True)

        fig = px.bar(
            df,
            x="Process Name",
            y="Disk Service Time",
            title=f"Top {top_charts_count} Disk Service Time per Process ({disk_name})",
            color="IO Type",
            color_discrete_map=io_type_color_mapping,
            barmode="group",
        )
        fig.update_xaxes(type="category")
        fig.update_layout(
            xaxis={"categoryorder": "total descending"},
            yaxis_title=f"Disk Service Time ({unit})",
        )
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("Show data"):
            st.dataframe(df)


def show_queue_depth_info(data: pd.DataFrame, latency_col: str):
    """Show Queue Depth (QD) statistics and charts."""

    # 1. Average QD per Disk
    df_qd_disk = (
        data.groupby("Disks")
        .apply(
            lambda g: pd.Series(
                {
                    "QD/I": (g["QD/I"] * g["Count"]).sum() / g["Count"].sum(),
                    "QD/C": (g["QD/C"] * g["Count"]).sum() / g["Count"].sum(),
                }
            )
        )
        .reset_index()
    )

    fig_qd = px.bar(
        df_qd_disk.melt(
            id_vars="Disks", var_name="Metric", value_name="Avg Queue Depth"
        ),
        x="Disks",
        y="Avg Queue Depth",
        color="Metric",
        barmode="group",
        title="Average Queue Depth per Disk (Init vs Complete)",
    )
    st.plotly_chart(fig_qd, use_container_width=True)

    # 2. Latency vs Queue Depth (Scatter Plot with binning)
    # We bin QD to make the chart readable
    data["QD_Bin"] = pd.cut(
        data["QD/C"],
        bins=[0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 9999],
        labels=[
            "1",
            "2",
            "3-4",
            "5-8",
            "9-16",
            "17-32",
            "33-64",
            "65-128",
            "129-256",
            ">256",
        ],
    )

    df_lat_qd = (
        data.groupby(["Disks", "QD_Bin"])
        .apply(
            lambda x: (
                (x[latency_col] * x["Count"]).sum() / x["Count"].sum()
                if x["Count"].sum() > 0
                else 0
            )
        )
        .reset_index(name="Avg Latency (µs)")
    )

    fig_lat_qd = px.line(
        df_lat_qd,
        x="QD_Bin",
        y="Avg Latency (µs)",
        color="Disks",
        markers=True,
        title=f"Average Latency vs Queue Depth (QD/C) [{latency_col}]",
        labels={"QD_Bin": "Queue Depth Range", "Avg Latency (µs)": latency_col},
    )
    st.plotly_chart(fig_lat_qd, use_container_width=True)

    with st.expander("Show QD Data"):
        st.dataframe(df_lat_qd)


def show_qos_analysis(data: pd.DataFrame, latency_col: str):
    """Show Quality of Service (QoS) analysis."""
    # Define latency buckets
    # <100us (Excellent), 100-500us (Good), 500us-1ms (Fair), 1-10ms (Poor), >10ms (Bad)
    bins = [0, 100, 500, 1000, 10000, 999999999]
    labels = ["<100µs", "100-500µs", "500µs-1ms", "1-10ms", ">10ms"]

    data["LatencyBucket"] = pd.cut(data[latency_col], bins=bins, labels=labels)

    # Calculate percentage of requests in each bucket per disk (Weighted by Count)
    df_qos = (
        data.groupby(["Disks", "LatencyBucket"])
        .apply(lambda x: x["Count"].sum())
        .reset_index(name="Request Count")
    )

    # Calculate percentages
    df_qos["Percent"] = (
        df_qos["Request Count"]
        / df_qos.groupby("Disks")["Request Count"].transform("sum")
        * 100
    )

    fig = px.bar(
        df_qos,
        x="Disks",
        y="Percent",
        color="LatencyBucket",
        title=f"Latency QoS Distribution (Percentage of I/Os) [{latency_col}]",
        color_discrete_map={
            "<100µs": "#00CC96",  # Green
            "100-500µs": "#636EFA",  # Blue
            "500µs-1ms": "#AB63FA",  # Purple
            "1-10ms": "#FFA15A",  # Orange
            ">10ms": "#EF553B",  # Red
        },
    )
    st.plotly_chart(fig, use_container_width=True)
    with st.expander("Show QoS Data"):
        st.dataframe(df_qos)


def show_misalignment_analysis(data: pd.DataFrame):
    """Show Misaligned I/O analysis."""
    # Check alignment (assuming 4096 sector size)
    data["IsMisaligned"] = data["Min Offset"] % 4096 != 0

    misaligned_data = data[data["IsMisaligned"] == True]

    if misaligned_data.empty:
        st.success("No misaligned I/O detected! (All I/Os are 4K aligned)")
        return

    num_misaligned = misaligned_data["Count"].sum()
    total_ios = data["Count"].sum()
    percent_misaligned = (num_misaligned / total_ios) * 100

    st.warning(
        f"⚠️ Detected {num_misaligned} misaligned I/O operations ({percent_misaligned:.2f}% of total)."
    )
    st.write("Misaligned I/Os cause Read-Modify-Write overhead on SSDs.")

    # Show culprits (Processes)
    df_culprits = (
        misaligned_data.groupby("Process Name")["Count"]
        .sum()
        .reset_index()
        .sort_values("Count", ascending=False)
        .head(10)
    )

    fig = px.bar(
        df_culprits,
        x="Process Name",
        y="Count",
        title="Top Processes causing Misaligned I/O",
        color="Process Name",
    )
    st.plotly_chart(fig, use_container_width=True)


def show_throughput_over_time(data: pd.DataFrame):
    """Show Throughput over time."""
    # Bin by integer second
    data["TimeBin"] = data["Init Time (s)"].astype(int)

    # Calculate MB/s per bin (Sum of Size / 1s)
    df_time = (
        data.groupby(["Disks", "TimeBin", "IO Type"])
        .apply(lambda g: (g["Count"] * g["Size (B)"]).sum())
        .reset_index(name="Size (B)")
    )
    df_time["Throughput (MB/s)"] = df_time["Size (B)"] / toMB

    fig = px.line(
        df_time,
        x="TimeBin",
        y="Throughput (MB/s)",
        color="Disks",
        line_dash="IO Type",
        title="Throughput Monitor (MB/s over Time)",
        labels={"TimeBin": "Time (seconds)"},
    )
    st.plotly_chart(fig, use_container_width=True)


def initial_sidebar_config():
    # sidebar contents
    sidebar = st.sidebar
    sidebar.subheader(":gear: Options Menu")
    return sidebar


def reset_filters(disks_names: list[str]):
    st.session_state.process_names = "ALL"
    st.session_state.avail_sizes = "ALL"
    st.session_state.selected_disks = disks_names
    st.session_state.percentage_threshold = 1.0
    st.session_state.top_charts_count = 10


@st.cache_resource
def download_custom_profile() -> Optional[bytes]:
    url = "https://raw.githubusercontent.com/bgeneto/windows-disk-trace-vis/main/DiskIO.wpaProfile"
    # Open the URL and read its content
    try:
        with urllib.request.urlopen(url) as response:
            custom_profile = response.read()
    except urllib.error.URLError:
        return None

    return custom_profile


def remove_disk(
    data: pd.DataFrame, disks_names: list[str], selected_disks: list[str]
) -> pd.DataFrame:
    """Remove disk from data."""
    for disk_name in disks_names:
        if disk_name not in selected_disks:
            data = data[data["Disks"] != disk_name]

    return data


# Define the Streamlit app
def main():
    global percentage_threshold
    global top_charts_count

    st.set_page_config(
        page_title=app_title,
        layout="wide",
        page_icon=":chart_with_upwards_trend:",
        initial_sidebar_state="auto",
    )

    # Set the page title
    st.title(f":chart_with_upwards_trend: {app_title}")

    # Set the sidebar contents
    sidebar = initial_sidebar_config()

    with st.expander("About this page"):
        st.header("About this page")
        st.write(summary)

    with st.expander("About WPR"):
        st.header("About WPR")
        st.write(about)

    with st.expander("Usage"):
        st.header(":computer: Usage")
        st.write(
            usage.format(
                python_svg=render_svg(python_svg, width="4%"),
                pandas_svg=render_svg(pandas_svg, width="2%"),
            ),
            unsafe_allow_html=True,
        )

    st.header(":inbox_tray: Download our profile")
    st.write(
        "You will need our custom WPA profile to run the `wpaexporter.exe` tool (see 'Usage' section above)."
    )
    custom_profile = download_custom_profile()
    st.download_button(
        "Download DiskIO.wpaProfile", custom_profile, "DiskIO.wpaProfile"
    )
    st.write(
        "A sample (Windows 11 boot) trace file can be downloaded [here](https://raw.githubusercontent.com/bgeneto/windows-disk-trace-vis/main/Disk_Usage_Counts_by_IO_Type_Priority.csv.bz2) for testing purposes."
    )
    st.header(":outbox_tray: Upload your log file")
    uploaded_file = st.file_uploader(
        "Upload your (compressed) wpaexporter generated csv file:",
        type=["csv", "zip", "gz", "bz2"],
        key="uploaded_file",
    )

    if uploaded_file is None:
        st.warning(
            "You have to upload your log file before we can continue...", icon="⚠️"
        )
        return

    # Read the uploaded log file
    start_time = timer()
    data = read_uploaded_file(uploaded_file)
    stop_time = timer()
    st.caption(f"Read and processing took: {stop_time - start_time:.2f} seconds.")

    # Display a sample of the loaded data
    with st.expander("Sample of the loaded data"):
        st.dataframe(data.head().style.format({"Time": "{:.6}"}))

    # Filter by process name
    process_names = sorted(data["Process Name"].unique().tolist())
    process_names.insert(0, "ALL")
    filter_process = sidebar.selectbox(
        "Filter by Process Name:", options=process_names, key="process_names"
    )
    avail_sizes = sorted(
        np.unique((data["Size (B)"].unique().astype(int) / toKB).astype(int)).tolist()
    )
    avail_sizes.pop(0)
    avail_sizes.insert(0, "ALL")
    filter_size = sidebar.selectbox(
        "Filter by Request Size (KB):",
        options=avail_sizes,
        key="avail_sizes",
    )

    # Rename disks if more than one disk is found
    selected_disks = None
    disks_names = sorted(data["Disks"].unique().tolist())
    if len(disks_names) >= 1:
        # check if the user selected a disk to remove
        placeholder = sidebar.empty()
        selected_disks = placeholder.multiselect(
            label="Disks to display:",
            options=disks_names,
            default=(
                st.session_state.selected_disks
                if "selected_disks" in st.session_state
                else disks_names
            ),
            key="selected_disks",
        )
        if selected_disks:
            data = remove_disk(data, disks_names, selected_disks)

    # customize percentage threshold
    percentage_threshold = sidebar.slider(
        "Percentage threshold:",
        0.0,
        5.0,
        value=1.0,
        step=0.25,
        key="percentage_threshold",
    )

    top_charts_count = sidebar.slider(
        "Number of top charts:",
        1,
        20,
        value=10,
        step=1,
        key="top_charts_count",
    )

    # reset options/filters button
    sidebar.button("Clear Filters", on_click=reset_filters, args=(disks_names,))

    # check again if there is more than one disk (after removing disks)
    disks_names = sorted(data["Disks"].unique().tolist())
    if len(disks_names) >= 1:
        # check if user wants to rename disks
        st.header(":pencil: Rename your disks")
        new_names = {}
        for name in disks_names:
            new_names[name] = st.text_input(name + " name:", name, key=name)
        # update data with new names
        data = update_disks_names(data, new_names)

    # Check if the user selected a process name
    if filter_process != "ALL":
        data = data[data["Process Name"] == filter_process]

    # Check if the user selected a size to filter
    if filter_size != "ALL":
        data = data[data["Size (B)"] == filter_size * toKB]

    # Show log summary
    st.header(":page_facing_up: Data Summary")
    st.table(log_summary(data))

    # Plot summary
    plot_summary(data)

    st.header(":bar_chart: Request Size Charts")
    with st.expander("Show info"):
        st.write(
            """
            The requested size is the amount of data requested by an application when reading/writing from/to a file.
            Typically, the requested size is a multiple of the sector size.

            The most important SSD performance metric for you will depend on your usage pattern.
            The charts below show the average request size and the number of requests for each size.
            This, along with the previously shown read/write and random/sequential ratios,
            can help you determine the most important aspect of an SSD disk to consider for your desired workload.
            """
        )
    show_request_size_count(data)
    show_request_size_histogram(data)
    show_request_size_time(data)
    show_request_size_bytes(data)

    latency_col = sidebar.radio(
        "Latency metric:",
        options=["Disk Service Time (µs)", "IO Time (µs)"],
        index=0,
        horizontal=True,
        help="Disk Service Time excludes queueing; IO Time includes queueing and is closer to end-to-end latency.",
    )

    # Show access time info
    st.header(":stopwatch: Performance Charts")
    with st.expander("Show info"):
        st.write(
            """
            Access time refers to the duration (Disk Service Time) it requires to read or write a specific portion of a file.
            In other words, it is the time the disk spent to complete a request of a particular length (not considering the Windows I/O queue delay).

            I don't know why but access time reported by WPR is smaller than the access time measured by other benchmarking apps like CDM or AS SSD.
            """
        )
    remove_outliers = sidebar.checkbox("Remove latency outliers", value=False)
    show_performance(
        data, filter_size, latency_col=latency_col, remove_outliers=remove_outliers
    )
    show_request_size_iops(data)
    show_request_size_process(data)
    show_service_time_process(data)

    st.header(":traffic_light: Queue Depth Analysis")
    with st.expander("Show info"):
        st.write(
            """
             **Queue Depth (QD)** represents the number of pending I/O requests at a given instant.
             *   **QD/I (Init)**: Queue depth when the request was issued.
             *   **QD/C (Complete)**: Queue depth when the request was completed.

             Modern NVMe SSDs are designed to handle high parallelism (High QD).
             If latency increases linearly with QD, the drive is saturating.
             """
        )
    show_queue_depth_info(data, latency_col=latency_col)

    st.header(":chart_with_downwards_trend: Advanced Diagnostics")
    st.subheader("1. Quality of Service (Latency QoS)")
    show_qos_analysis(data, latency_col=latency_col)

    st.subheader("2. I/O Alignment Analysis")
    show_misalignment_analysis(data)

    st.subheader("3. Temporal Analysis (Throughput over Time)")
    show_throughput_over_time(data)


if __name__ == "__main__":
    # Increase pandas default output precision
    pd.set_option("display.precision", 7)
    pd.options.display.float_format = "{:.7f}".format

    # Set column names
    column_names = [
        "IO Type",
        "Priority",
        "Process Name",
        "Init Time (s)",
        "Complete Time (s)",
        "IO Time (µs)",
        "Disk Service Time (µs)",
        "Size (B)",
        "Min Offset",
        "Max Offset",
        "QD/I",
        "QD/C",
        "Disks",
        "Count",
    ]

    column_types = {
        "Count": int,
        "Disks": int,
        "QD/C": int,
        "QD/I": int,
        "Max Offset": str,
        "Min Offset": str,
        "Size (B)": str,
        "Disk Service Time (µs)": str,
        "IO Time (µs)": str,
        "Complete Time (s)": str,
        "Init Time (s)": str,
        "Process Name": str,
        "Priority": str,
        "IO Type": str,
    }

    # Define specific request sizes to account for
    length_values = [
        512,
        1024,
        2048,
        4096,
        8192,
        16384,
        32768,
        65536,
        131072,
        262144,
        524288,
        1048576,
        2097152,
        4194304,
    ]

    # color mappings
    io_type_color_mapping = {
        "Read": "#636EFA",
        "Write": "#EF553B",
    }
    category_color_mapping = {
        "SEQ": "#00CC96",
        "RND": "#AB63FA",
    }

    # defaults
    percentage_threshold = 1.0
    top_charts_count = 10

    # conversion factors from bytes
    toGB = 1024**3
    toMB = 1024**2
    toKB = 1024

    # conversion factors from microseconds
    toMSec = 1e3
    toSec = 1e6

    # Run the app
    t0 = timer()
    main()
    tf = timer()
    st.caption(f"Total computational time: {tf-t0:.2f} seconds.")
