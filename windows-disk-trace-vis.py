"""Windows Disk Trace Visualizer.

A simple but useful web application designed to streamline the process of analyzing Event Trace Log (ETL) files,
specifically focusing on Disk I/O activities recorded by Windows Performance Recorder (or Xperf).

Author:   b g e n e t o @ g m a i l . c o m
History:  v1.0.0 Initial release
          v1.0.1 Updated profile link
          v1.0.2 Removed babel dependency and added locale_adjust_numbers function (faster)
                 Added custom profile download button
          v1.0.3 Split request size graphics into random and sequential categories.
                 Added option to remove disks.
          v1.0.4 Added Request Size by Duration chart.
Modified: 20230903
Usage:
    $ streamlit run windows-disk-trace-vis.py
"""

__VERSION__ = "1.0.4"

import base64
import urllib.error
import urllib.request
from timeit import default_timer as timer
from typing import Optional

import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st

app_title = "Windows Disk Trace Visualizer"

about = """
Windows Performance Recorder ([WPR](https://learn.microsoft.com/en-us/windows-hardware/test/wpt/windows-performance-recorder))
is a performance recording tool that is based on Event Tracing for Windows ([ETW](https://learn.microsoft.com/en-us/windows/win32/etw/about-event-tracing)).
It records system and application events that you can analyze by parsing the generated Event Trace Log ([ETL](https://learn.microsoft.com/en-us/windows-hardware/drivers/devtest/trace-log)) file.
The ETL file format is a proprietary binary file format used by Microsoft Windows for collecting
and storing event traces generated by various components of the operating system. Since it is a poorly documented and
proprietary format, we need to use the [WPA Exporter](https://learn.microsoft.com/en-us/windows-hardware/test/wpt/exporter)
tool in order to convert the .etl file to a .csv file and then perform the desired trace analysis via csv file. Both WPR and WPA Exporter are part of the
Windows Assessment and Deployment Kit [Windows ADK](https://learn.microsoft.com/en-us/windows-hardware/get-started/adk-install).
"""

summary = f"""
"{app_title}" is a simple but useful web application designed to streamline the process of analyzing Event Trace Log (ETL) files,
specifically focusing on Disk I/O activities. This tool offers an intuitive and user-friendly interface to effortlessly navigate
through complex ETL data, providing insightful summaries and statistics for a thorough understanding of your disk performance requirements.
Key Features:
- **Data Analysis:** Uncover the most frequently requested data size, the percentage of random reading or writing, the average access time, and more.
- **Performance:** Identify the average throughput and IOPS for each disk, and compare the performance of sequential and random requests.
- **Filter Capability:** Filter data by process name or request size.

All this info is valuable for understanding where the performance of your disk/ssd matters most for your desired/traced workload/application.

Version {__VERSION__} made with [Streamlit](https://streamlit.io/) with ❤️ by [bgeneto](https://github.com/bgeneto/).
"""

python_svg = """
<svg width="800px" height="800px" viewBox="0 0 64 64" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M31.885 16c-8.124 0-7.617 3.523-7.617 3.523l.01 3.65h7.752v1.095H21.197S16 23.678 16 31.876c0 8.196 4.537 7.906 4.537 7.906h2.708v-3.804s-.146-4.537 4.465-4.537h7.688s4.32.07 4.32-4.175v-7.019S40.374 16 31.885 16zm-4.275 2.454c.771 0 1.395.624 1.395 1.395s-.624 1.395-1.395 1.395a1.393 1.393 0 0 1-1.395-1.395c0-.771.624-1.395 1.395-1.395z" fill="url(#a)"/><path d="M32.115 47.833c8.124 0 7.617-3.523 7.617-3.523l-.01-3.65H31.97v-1.095h10.832S48 40.155 48 31.958c0-8.197-4.537-7.906-4.537-7.906h-2.708v3.803s.146 4.537-4.465 4.537h-7.688s-4.32-.07-4.32 4.175v7.019s-.656 4.247 7.833 4.247zm4.275-2.454a1.393 1.393 0 0 1-1.395-1.395c0-.77.624-1.394 1.395-1.394s1.395.623 1.395 1.394c0 .772-.624 1.395-1.395 1.395z" fill="url(#b)"/><defs><linearGradient id="a" x1="19.075" y1="18.782" x2="34.898" y2="34.658" gradientUnits="userSpaceOnUse"><stop stop-color="#387EB8"/><stop offset="1" stop-color="#366994"/></linearGradient><linearGradient id="b" x1="28.809" y1="28.882" x2="45.803" y2="45.163" gradientUnits="userSpaceOnUse"><stop stop-color="#FFE052"/><stop offset="1" stop-color="#FFC331"/></linearGradient></defs></svg>
"""
pandas_svg = """
<svg data-name="Layer 1" version="1.1" viewBox="0 0 210.21 280.43" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<metadata><rdf:RDF><cc:Work rdf:about=""><dc:format>image/svg+xml</dc:format><dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage"/></cc:Work></rdf:RDF></metadata>
<defs><style>.cls-1{fill:#130754;}.cls-2{fill:#48e5ac;}.cls-3{fill:#e70488;}</style></defs>
<rect class="cls-1" x="74.51" y="43.03" width="24.09" height="50.02"/>
<rect class="cls-1" x="74.51" y="145.78" width="24.09" height="50.02"/>
<rect class="cls-2" x="74.51" y="107.65" width="24.09" height="23.6" fill="#ffca00"/>
<rect class="cls-1" x="35.81" y="84.15" width="24.09" height="166.27"/>
<rect class="cls-1" x="112.41" y="187.05" width="24.09" height="50.02"/>
<rect class="cls-1" x="112.41" y="84.21" width="24.09" height="50.02"/>
<rect class="cls-3" x="112.41" y="148.84" width="24.09" height="23.6"/>
<rect class="cls-1" x="150.3" y="30" width="24.09" height="166.27"/>
</svg>
"""

usage = """
[YouTube Quick Tutorial](https://youtu.be/XgtjpdunUoI)

First, you need to record a trace of your 'Disk I/O activity' with WPRUI.exe. You can trace your Windows boot process or a specific application/workload.
Then you need to convert the saved `.etl` file to a `.csv` file using the `wpaexporter.exe` tool that comes with [Windows ADK](https://learn.microsoft.com/en-us/windows-hardware/get-started/adk-install).
The `wpaexporter.exe` tool is typically located in the `C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit` folder. An example of usage follows:
```cmd
wpaexporter.exe -i boottrace.etl -profile DiskIO.wpaProfile -delimiter ;
```

If you want to trace your boot process do as follows:

- Open an elevated command prompt and run (change the path to your desired location):

   ```
   wpr -addboot GeneralProfile.Light -filemode -recordtempto D:\\Temp
   ```

After this, the trace will start automatically at the early stage of the next (re)boot.

- The command syntax to save the boot trace (.etl file) &mdash; after boot completion &mdash; is the following:

   ```
   wpr -stopboot D:\\Temp\\boottrace.etl
   ```

Now you can upload the (compressed) `.csv` file to this page. The page script &mdash; written in Python{python_svg}using Pandas{pandas_svg} &mdash; will analyze the single trace record using the provided
[profile](https://raw.githubusercontent.com/bgeneto/windows-disk-trace-vis/main/DiskIO.wpaProfile) and show you the results in tabular and graphical formats.
> **Note:** tested with WPR v10.0.25931.1000 on Windows 11 Pro version 22H2.
"""


def render_svg(svg, width="100%", height="100%") -> str:
    """Renders the given svg string."""
    b64 = base64.b64encode(svg.encode("utf-8")).decode("utf-8")
    html = (
        rf'<img width={width} height={height} src="data:image/svg+xml;base64,{b64}"/>'
    )
    return html


def locale_adjust_numbers(data: pd.DataFrame) -> pd.DataFrame:
    """Check number format and adjust data accordingly."""
    int_cols = ["Size (B)"]
    float_cols = [
        "Init Time (s)",
        "Complete Time (s)",
        "Disk Service Time (µs)",
        "IO Time (µs)",
    ]

    # remove spaces, dots and commas from integer columns
    for col in int_cols:
        data[col] = (
            data[col].str.replace(" ", "").str.replace(".", "").str.replace(",", "")
        ).astype(int)

    # locale aware format float columns
    if data["Init Time (s)"][:10].str.contains(",").all():
        for col in float_cols:
            data[col] = (
                data[col]
                .str.replace(" ", "")
                .str.replace(".", "")
                .str.replace(",", ".")
                .astype(float)
            )
    else:
        for col in float_cols:
            data[col] = (
                data[col].str.replace(" ", "").str.replace(",", "").astype(float)
            )

    return data


@st.cache_data
def read_uploaded_file(uploaded_file) -> pd.DataFrame:
    """Read the uploaded file using pandas."""

    # check if the uploaded file is compressed
    compression = None
    if uploaded_file.name.endswith(".zip"):
        compression = "zip"
    elif uploaded_file.name.endswith(".gz"):
        compression = "gzip"
    elif uploaded_file.name.endswith(".bz2"):
        compression = "bz2"

    # Read the uploaded file using pandas
    data = pd.read_csv(
        uploaded_file,
        sep=";",
        header=None,
        names=column_names,
        dtype=column_types,
        compression=compression,
        skiprows=1,
    )

    # Drop rows where "IO Type" is not "Read" or "Write"
    data = data[data["IO Type"].isin(["Read", "Write"])]

    # Drop rows where "Priority" is not "Normal"
    data = data[data["Priority"] == "Normal"]

    data = locale_adjust_numbers(data)

    # Order the DataFrame by "Init Time (s)" column in order to calculate the difference between max offsets and min offsets
    # and categorize the operation type as "SEQ" or "RND"
    data = data.sort_values(by=["Init Time (s)"], ignore_index=True)

    # Convert "Disks" to string so plotly can recognize it as categorical (not numerical, continuous)
    data["Disks"] = "Disk " + data["Disks"].astype(str)

    # Convert "Min Offset" and "Max Offset" from hex to int
    data["Min Offset"] = data["Min Offset"].apply(lambda x: int(x, 16))
    data["Max Offset"] = data["Max Offset"].apply(lambda x: int(x, 16))

    # Classify the operation type as "SEQ" or "RND" by comparing the "Max Offset" from previous row with the "Min Offset" from current row
    # Calculate the difference between max offsets and min offsets
    data["Category"] = np.where(
        data["Min Offset"] == data["Max Offset"].shift(1) + 1, "SEQ", "RND"
    )

    return data.dropna(how="any")


@st.cache_data
def update_disks_names(data: pd.DataFrame, names: dict) -> pd.DataFrame:
    """Change disk name."""
    for old_name, new_name in names.items():
        data.loc[data["Disks"] == old_name, "Disks"] = new_name

    return data


def log_summary(data: pd.DataFrame) -> pd.DataFrame:
    """Show totals."""

    summary = {}

    # Compute the total monitoring time by taking the max value from the "Time" column
    summary["Monitoring time"] = "{:.2f} seconds".format(
        data["Complete Time (s)"].max()
    )

    # average time for each operation
    summary["Average access time"] = "{:.2f} µs".format(
        data["Disk Service Time (µs)"].mean()
    )

    # number of read and write requests
    summary["Read requests"] = data[data["IO Type"] == "Read"].shape[0]
    summary["Write requests"] = data[data["IO Type"] == "Write"].shape[0]

    # total requests
    summary["Total requests"] = summary["Read requests"] + summary["Write requests"]

    # percentage of read and write requests
    summary["Percent READ"] = "{:.2f}%".format(
        summary["Read requests"] / summary["Total requests"] * 100
    )
    summary["Percent WRITE"] = "{:.2f}%".format(
        summary["Write requests"] / summary["Total requests"] * 100
    )

    # total percentage SEQ and RND requests
    summary["Percent RANDOM"] = "{:.2f}%".format(
        data[data["Category"] == "RND"].shape[0] / summary["Total requests"] * 100
    )
    summary["Percent SEQUENTIAL"] = "{:.2f}%".format(
        data[data["Category"] == "SEQ"].shape[0] / summary["Total requests"] * 100
    )

    # total data read in GBytes
    summary["Read data size"] = "{:.2f} GB".format(
        ((data["Count"] * data["Size (B)"]).where(data["IO Type"] == "Read")).sum()
        / toGB
    )

    # total data written in GBytes
    summary["Write data size"] = "{:.2f} GB".format(
        (data["Count"] * data[data["IO Type"] == "Write"]["Size (B)"]).sum() / toGB
    )

    # total data size in GBytes
    summary["Total data size"] = "{:.2f} GB".format(
        (data["Count"] * data["Size (B)"]).sum() / toGB
    )

    # min and max readrequests in KB
    summary["Min. read request size"] = "{:.1f} KB".format(
        (data["Count"] * data[data["IO Type"] == "Read"]["Size (B)"]).min() / toKB
    )

    # avg read request in KB
    summary["Avg. read request size"] = "{:.1f} KB".format(
        (data["Count"] * data[data["IO Type"] == "Read"]["Size (B)"]).mean() / toKB
    )

    # max read requests in KB
    number = (data["Count"] * data[data["IO Type"] == "Read"]["Size (B)"]).max() / toKB
    formatted_number = (
        "{:.1f}".format(number) if number % 1 else "{:.0f}".format(number)
    )
    summary["Max. read request size"] = formatted_number + " KB"

    # min and max write requests in KB
    summary["Min. write request size"] = "{:.1f} KB".format(
        (data["Count"] * data[data["IO Type"] == "Write"]["Size (B)"]).min() / toKB
    )

    # avg write request in KB
    summary["Avg. write request size"] = "{:.1f} KB".format(
        (data["Count"] * data[data["IO Type"] == "Write"]["Size (B)"]).mean() / toKB
    )

    # max write requests in KB
    number = (data["Count"] * data[data["IO Type"] == "Write"]["Size (B)"]).max() / toKB
    formatted_number = (
        "{:.1f}".format(number) if number % 1 else "{:.0f}".format(number)
    )
    summary["Max. write request size"] = formatted_number + " KB"

    return pd.DataFrame.from_dict(summary, orient="index", columns=["Value"])


def plot_summary(data: pd.DataFrame):
    # Total requests in GB

    df = (
        (data.groupby(["Disks", "IO Type"])["Size (B)"].sum() / toGB)
        .to_frame()
        .reset_index()
    )

    # Calculate percentage within each disk group
    df["Percent"] = (
        df["Size (B)"] / df.groupby("Disks")["Size (B)"].transform("sum") * 100
    )

    df.rename(columns={"Size (B)": "Size (GB)"}, inplace=True)

    # Create a plotly bar chart
    fig = px.bar(
        df,
        x="Disks",
        y="Size (GB)",
        title="Requested Data Size",
        color="IO Type",
        barmode="group",
        text="Percent",
    )

    # Annotate the bars with percentage values
    fig.update_traces(texttemplate="%{text:.3s}%", textposition="inside")

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)
        st.write(
            '> **Note:** There is a difference between the percentages shown on this chart and those that are represented as "Percent READ" and "Percent WRITE" in the summary table above.'
            + " The difference between the two is that the chart shows the percentage of data size, whilst the table shows the percentage of (the number of) requests. "
            + " Numerous (short length) queries may result in a higher number of requests but a lesser amount of data being read or written since the size (in Bytes) of each request can vary."
        )

    # RND and SEQ requested data size in GB
    # --------------------------------------
    df = (
        (data.groupby(["Disks", "Category"])["Size (B)"].sum() / toGB)
        .to_frame()
        .reset_index()
    )

    # Calculate percentage within each disk group
    df["Percent"] = (
        df["Size (B)"] / df.groupby("Disks")["Size (B)"].transform("sum") * 100
    )

    df.rename(columns={"Size (B)": "Size (GB)"}, inplace=True)

    # Create a plotly bar chart
    fig = px.bar(
        df,
        x="Disks",
        y="Size (GB)",
        title="Requested Data Size by Category (Random/Sequential)",
        color="Category",
        barmode="group",
        text="Percent",
    )

    # Annotate the bars with percentage values
    fig.update_traces(texttemplate="%{text:.3s}%", textposition="inside")

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)
        st.write(
            '> **Note:** The percentage displayed on this chart is distinct from the "Percent RANDOM" and "Percent SEQUENTIAL" indicated in '
            + "the Data Summary table above. This distinction arises because the table presents the percentage of RND/SEQ requests, whereas the chart illustrates "
            + "the percentage based on data size."
        )

    # RND and SEQ requested data by request type per disk
    # --------------------------------------
    disks_names = sorted(data["Disks"].unique().tolist())
    for disk_name in disks_names:
        df = (
            (
                data[data["Disks"] == disk_name]
                .groupby(["IO Type", "Category"])["Size (B)"]
                .sum()
                / toGB
            )
            .to_frame()
            .reset_index()
        )

        # Calculate percentage within each disk group
        df["Percent"] = (
            df["Size (B)"] / df.groupby("Category")["Size (B)"].transform("sum") * 100
        )

        df.rename(columns={"Size (B)": "Size (GB)"}, inplace=True)

        # Create a plotly bar chart
        fig = px.bar(
            df,
            x="Category",
            y="Size (GB)",
            title=f"Requested Data Size by Category (RND/SEQ) and IO Type (R/W) ({disk_name})",
            color="IO Type",
            barmode="group",
            text="Percent",
        )

        # Annotate the bars with percentage values
        fig.update_traces(texttemplate="%{text:.3s}%", textposition="inside")

        st.plotly_chart(fig, use_container_width=True)

        with st.expander("Show data"):
            st.dataframe(df)


def outlier_thresholds_iqr(data: pd.DataFrame, col_name: str, lth=0.05, uth=0.95):
    """
    Calculates the lower and upper outlier thresholds using the interquartile range (IQR) method.

    Args:
        data (pd.DataFrame): The input DataFrame.
        col_name (str): The column name from which to calculate the thresholds.
        th1 (float): The lower percentile for the quartile calculation. Defaults to 0.05.
        th3 (float): The upper percentile for the quartile calculation. Defaults to 0.95.

    Returns:
        float: The lower outlier threshold.
        float: The upper outlier threshold.
    """
    quartile1 = data[col_name].quantile(lth)
    quartile3 = data[col_name].quantile(uth)
    iqr = quartile3 - quartile1
    upper_limit = quartile3 + 1.5 * iqr
    lower_limit = quartile1 - 1.5 * iqr
    return lower_limit, upper_limit


def show_performance(data: pd.DataFrame, filter_size, remove_outliers: bool = False):
    """Compute disk access time and other performance metrics."""
    # Remove outliers
    fdf = data
    if remove_outliers:
        _, ub = outlier_thresholds_iqr(data, "Disk Service Time (µs)", 0.01, 0.99)

        # Filter the DataFrame based on quartiles
        fdf = data[
            (data["Disk Service Time (µs)"] > 0)
            & (data["Disk Service Time (µs)"] <= ub)
        ]

    # average access time by disk
    df = fdf.groupby(["Disks"])[
        "Disk Service Time (µs)"
    ].mean()  # Reset index to access "Disks" as a column

    # Create Plotly bar plot
    fig = px.bar(
        df.reset_index(),
        x="Disks",
        y="Disk Service Time (µs)",
        title="Average Access Time",
        color="Disks",
        labels={"Disk Service Time (µs)": "Average Access Time (µs)"},
    )

    # Set x-axis tickmode to "array" and provide the index values to only show those
    fig.update_xaxes(tickmode="array", tickvals=df.index)

    # Add unit to hover text using hovertemplate
    fig.update_traces(
        hovertemplate="Disk: %{x}<br>Average Time: %{y:.2f} µs<extra></extra>"
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average by disk and by request type
    df = fdf.groupby(["Disks", "IO Type"])["Disk Service Time (µs)"].mean()

    # Create an interactive grouped bar plot using Plotly
    fig = px.bar(
        df.unstack(),
        color="IO Type",
        barmode="group",
        title="Average Access Time per IO Type",
        labels={"value": "Average Access Time (µs)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average throughput by disk
    total_time_sec = fdf.groupby(["Disks"])["Disk Service Time (µs)"].sum() * 1e-6
    df = (fdf.groupby(["Disks"])["Size (B)"].sum() / toMB) / total_time_sec
    df.rename("Average Throughput (MB/s)", inplace=True)

    fig = px.bar(
        df.reset_index(),
        x="Disks",
        y="Average Throughput (MB/s)",
        color="Disks",
        title="Average Throughput",
        labels={"value": "Average Throughput (MB/s)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average throughput by disk and by IO type
    total_time_sec = (
        fdf.groupby(["Disks", "IO Type"])["Disk Service Time (µs)"].sum() * 1e-6
    )
    df = (fdf.groupby(["Disks", "IO Type"])["Size (B)"].sum() / toMB) / total_time_sec

    # Create an interactive grouped bar plot using Plotly
    fig = px.bar(
        df.unstack(),
        color="IO Type",
        barmode="group",
        title="Average Throughput per IO Type",
        labels={"value": "Average Throughput (MB/s)"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # average throughput per category
    disks_names = sorted(data["Disks"].unique().tolist())
    for disk_name in disks_names:
        total_time_sec = (
            fdf[fdf["Disks"] == disk_name]
            .groupby(["IO Type", "Category"])["Disk Service Time (µs)"]
            .sum()
            * 1e-6
        )

        df = (
            fdf[data["Disks"] == disk_name]
            .groupby(["IO Type", "Category"])["Size (B)"]
            .sum()
            / toMB
        ) / total_time_sec

        df.rename("Average Throughput (MB/s)", inplace=True)

        # Create an interactive grouped bar plot using Plotly
        fig = px.bar(
            df.reset_index(),
            x="Category",
            y="Average Throughput (MB/s)",
            color="IO Type",
            barmode="group",
            title=f"Average Throughput per IO Type and Category ({disk_name})",
            labels={"value": "Average Throughput (MB/s)"},
        )

        st.plotly_chart(fig, use_container_width=True)

        with st.expander("Show data"):
            st.dataframe(df)

    # average IOPS
    total_time_sec = (
        fdf.groupby(["Disks", "IO Type"])["Disk Service Time (µs)"].sum() * 1e-6
    )
    df = fdf.groupby(["Disks", "IO Type"])["Size (B)"].count() / total_time_sec

    # Create an interactive grouped bar plot using Plotly
    fig = px.bar(
        df.unstack(),
        color="IO Type",
        barmode="group",
        title=f"Average IOPS (size: {filter_size} KB)",
        labels={"value": "Average IOPS"},
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)


def custom_int_float_format(value):
    return "{:.1f}".format(value) if value % 1 else "{:.0f}".format(value)


def show_request_size_count(data: pd.DataFrame):
    """Show most frequent request size per total request count."""
    # compute average read and write request size in kbytes
    df = (
        (data.groupby(["Disks", "IO Type"])["Size (B)"].mean() / toKB)
        .to_frame()
        .reset_index()
    )

    # rename the columns
    df.rename(columns={"Size (B)": "Avg. Size (KB)"}, inplace=True)

    # Create Plotly bar plot
    fig = px.bar(
        df,
        x="Disks",
        y="Avg. Size (KB)",
        color="IO Type",
        barmode="group",
        title="Average Request Size",
    )

    st.plotly_chart(fig, use_container_width=True)

    with st.expander("Show data"):
        st.dataframe(df)

    # Count occurrences of specific "Size (B)"
    length_counts_df = (
        data.groupby(["Disks", "IO Type"])["Size (B)"]
        .value_counts()
        .sort_index()
        .to_frame()
        .reset_index()
    )

    for disk_name in sorted(length_counts_df["Disks"].unique().tolist()):
        df = length_counts_df[length_counts_df["Disks"] == disk_name].copy()
        df["Request Size"] = (
            (df["Size (B)"] / toKB)
            .apply(lambda x: "{:.1f}KB".format(x) if x % 1 else "{:.0f}KB".format(x))
            .astype(str)
        )
        df.drop(["Disks", "Size (B)"], axis=1, inplace=True)
        # Calculate percentage within each request group
        df["Percent"] = (
            df["count"] / df.groupby("IO Type")["count"].transform("sum") * 100
        )
        # Identify rows to drop
        rows_to_drop = df.index[df["Percent"] < percentage_threshold]
        # Drop rows
        df.drop(index=rows_to_drop, inplace=True)
        # Create the plot
        fig = px.bar(
            df.sort_values(by=["Percent"], ascending=False),
            x="Request Size",
            y="Percent",
            color="IO Type",
            barmode="group",
            title=f"Percentage of Number of Requests per Request Size ({disk_name})",
            text="Percent",
            custom_data=["IO Type", "Percent", "count"],
        )
        # Annotate the bars with percentage values
        fig.update_xaxes(type="category")
        fig.update_traces(
            texttemplate="%{text:.3s}%",
            textposition="inside",
            hovertemplate="Size: %{x}<br>Type: %{customdata[0]}<br>Percent: %{customdata[1]:.1f}%<br>Count: %{customdata[2]}<extra></extra>",
        )
        fig.update_layout(
            xaxis={"categoryorder": "total descending"},
            yaxis_title="Percent of Requests",
        )
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("Show data"):
            st.dataframe(df)


def show_request_size_bytes(data: pd.DataFrame):
    """Show most frequent request size per total bytes."""

    # Count occurrences of specific "Size (B)"
    length_bytes_df = (
        data.groupby(["Disks", "IO Type"])["Size (B)"]
        .value_counts()
        .sort_index()
        .to_frame()
        .reset_index()
    )
    length_bytes_df["Total Size"] = (
        length_bytes_df["Size (B)"] * length_bytes_df["count"]
    )

    # find the best unit to display the total size by averaging the total size
    # and then dividing by the best unit
    avg = length_bytes_df["Total Size"].mean()
    if avg / toGB > 1:
        length_bytes_df["Total Size"] = length_bytes_df["Total Size"] / toGB
        unit = "GB"
    elif avg / toMB > 1:
        length_bytes_df["Total Size"] = length_bytes_df["Total Size"] / toMB
        unit = "MB"
    elif avg / toKB > 1:
        length_bytes_df["Total Size"] = length_bytes_df["Total Size"] / toKB
        unit = "KB"
    else:
        length_bytes_df["Total Size"] = length_bytes_df["Total Size"]
        unit = "B"

    for disk_name in sorted(length_bytes_df["Disks"].unique().tolist()):
        df = length_bytes_df[length_bytes_df["Disks"] == disk_name].copy()
        df["Request Size"] = (
            (df["Size (B)"] / toKB)
            .apply(lambda x: "{:.1f}KB".format(x) if x % 1 else "{:.0f}KB".format(x))
            .astype(str)
        )
        # Calculate percentage within each request group
        df["Percent"] = (
            df["Total Size"]
            / df.groupby("IO Type")["Total Size"].transform("sum")
            * 100
        )
        # drop unused columns
        df.drop(["Disks", "Size (B)", "count"], axis=1, inplace=True)
        # Identify rows to drop
        rows_to_drop = df.index[df["Percent"] < percentage_threshold]
        # Drop rows
        df.drop(index=rows_to_drop, inplace=True)
        # Create the plot
        fig = px.bar(
            df.sort_values(by=["Percent"], ascending=False),
            x="Request Size",
            y="Percent",
            color="IO Type",
            barmode="group",
            title=f"Percentage of R/W Data Size per Request Size ({disk_name})",
            text="Percent",
            custom_data=["IO Type", "Percent", "Total Size"],
        )
        # Annotate the bars with percentage values
        fig.update_xaxes(type="category")
        fig.update_traces(
            texttemplate="%{text:.3s}%",
            textposition="inside",
            hovertemplate="Size: %{x}<br>Type: %{customdata[0]}<br>Percent: %{customdata[1]:.1f}%<br>Size: %{customdata[2]:.1f}"
            + unit
            + "<extra></extra>",
        )
        fig.update_layout(
            xaxis={"categoryorder": "total descending"},
            yaxis_title="Percent of Requested Data",
        )
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("Show data"):
            st.dataframe(df)
            st.write(
                f"> **Note:** Percentages below the threshold ({percentage_threshold}%) are not displayed. "
                + "The slider in the :point_left: left sidebar allows you to modify this threshold value. "
                + "Decrease the threshold to show more data, or increase it to show less data."
            )


def show_request_size_time(data: pd.DataFrame):
    """Show request sizes vs disk service time."""

    # One chart for each disk
    for disk_name in sorted(data["Disks"].unique().tolist()):
        df = data[data["Disks"] == disk_name].copy()
        df = (
            (
                df.groupby(["IO Type", "Category", "Size (B)"])[
                    "Disk Service Time (µs)"
                ].sum()
            )
            .to_frame()
            .reset_index()
        )

        # rename column
        df.rename(columns={"Disk Service Time (µs)": "Disk Service Time"}, inplace=True)

        # request size with proper unit
        df["Request Size"] = (
            (df["Size (B)"] / toKB)
            .apply(lambda x: "{:.1f}KB".format(x) if x % 1 else "{:.0f}KB".format(x))
            .astype(str)
        )

        # Calculate percentage within each disk group
        df["Percent"] = (
            df["Disk Service Time"]
            / df.groupby("IO Type")["Disk Service Time"].transform("sum")
            * 100
        )

        # find proper unit for disk service time
        avg = df["Disk Service Time"].mean()
        if avg / toSec > 1:
            df["Disk Service Time"] = df["Disk Service Time"] / toSec
            unit = "s"
        elif avg / toMSec > 1:
            df["Disk Service Time"] = df["Disk Service Time"] / toMSec
            unit = "ms"
        else:
            unit = "µs"

        df.drop(["Size (B)"], axis=1, inplace=True)

        # Drop rows
        rows_to_drop = df.index[df["Percent"] < percentage_threshold]
        df.drop(index=rows_to_drop, inplace=True)

        # Create a plotly bar chart
        fig = px.bar(
            df.sort_values(by=["Percent"], ascending=False),
            x="Request Size",
            y="Percent",
            title=f"Disk Service Time per Request Size ({disk_name})",
            color="IO Type",
            barmode="group",
            text="Percent",
            custom_data=["IO Type", "Percent", "Disk Service Time", "Category"],
        )

        # Annotate the bars with percentage values
        fig.update_xaxes(type="category")
        fig.update_traces(
            texttemplate="%{text:.3s}%",
            textposition="inside",
            hovertemplate="Size: %{x}<br>Type: %{customdata[0]}<br>Percent: %{customdata[1]:.1f}%<br>Time: %{customdata[2]:.1f}"
            + unit
            + "<br>Category: %{customdata[3]}<extra></extra>",
        )
        fig.update_layout(
            xaxis={"categoryorder": "total descending"},
            yaxis_title="Percent of Disk Service Time",
        )
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("Show data"):
            st.dataframe(df)


def initial_sidebar_config():
    # sidebar contents
    sidebar = st.sidebar
    sidebar.subheader(":gear: Options Menu")
    return sidebar


def reset_filters(disks_names: list[str]):
    st.session_state.process_names = "ALL"
    st.session_state.avail_sizes = "ALL"
    st.session_state.selected_disks = disks_names
    st.session_state.percentage_threshold = 1.0


@st.cache_resource
def download_custom_profile() -> Optional[bytes]:
    url = "https://raw.githubusercontent.com/bgeneto/windows-disk-trace-vis/main/DiskIO.wpaProfile"
    # Open the URL and read its content
    try:
        with urllib.request.urlopen(url) as response:
            custom_profile = response.read()
    except urllib.error.URLError:
        return None

    return custom_profile


@st.cache_data
def remove_disk(
    data: pd.DataFrame, disks_names: list[str], selected_disks: list[str]
) -> pd.DataFrame:
    """Remove disk from data."""
    for disk_name in disks_names:
        if disk_name not in selected_disks:
            data = data[data["Disks"] != disk_name]

    return data


# Define the Streamlit app
def main():
    global percentage_threshold

    st.set_page_config(
        page_title=app_title,
        # layout="wide",
        page_icon=":chart_with_upwards_trend:",
        initial_sidebar_state="auto",
    )

    # Set the page title
    st.title(f":chart_with_upwards_trend: {app_title}")

    # Set the sidebar contents
    sidebar = initial_sidebar_config()

    with st.expander("About this page"):
        st.header("About this page")
        st.write(summary)

    with st.expander("About WPR"):
        st.header("About WPR")
        st.write(about)

    with st.expander("Usage"):
        st.header(":computer: Usage")
        st.write(
            usage.format(
                python_svg=render_svg(python_svg, width="4%"),
                pandas_svg=render_svg(pandas_svg, width="2%"),
            ),
            unsafe_allow_html=True,
        )

    st.header(":inbox_tray: Download our profile")
    st.write(
        "You will need our custom WPA profile to run the `wpaexporter.exe` tool (see 'Usage' section above)."
    )
    custom_profile = download_custom_profile()
    st.download_button(
        "Download DiskIO.wpaProfile", custom_profile, "DiskIO.wpaProfile"
    )
    st.write(
        "A sample (Windows 11 boot) trace file can be downloaded [here](https://raw.githubusercontent.com/bgeneto/windows-disk-trace-vis/main/Disk_Usage_Counts_by_IO_Type_Priority.csv.bz2) for testing purposes."
    )
    st.header(":outbox_tray: Upload your log file")
    uploaded_file = st.file_uploader(
        "Upload your (compressed) wpaexporter generated csv file:",
        type=["csv", "zip", "gz", "bz2"],
        key="uploaded_file",
    )

    if uploaded_file is None:
        st.warning(
            "You have to upload your log file before we can continue...", icon="⚠️"
        )
        return

    # Read the uploaded log file
    start_time = timer()
    data = read_uploaded_file(uploaded_file)
    stop_time = timer()
    st.caption(f"Read and processing took: {stop_time - start_time:.2f} seconds.")

    # Display a sample of the loaded data
    with st.expander("Sample of the loaded data"):
        st.dataframe(data.head().style.format({"Time": "{:.6}"}))

    # Filter by process name
    process_names = sorted(data["Process Name"].unique().tolist())
    process_names.insert(0, "ALL")
    filter_process = sidebar.selectbox(
        "Filter by Process Name:", options=process_names, key="process_names"
    )
    avail_sizes = sorted(
        np.unique((data["Size (B)"].unique().astype(int) / toKB).astype(int)).tolist()
    )
    avail_sizes.pop(0)
    avail_sizes.insert(0, "ALL")
    filter_size = sidebar.selectbox(
        "Filter by Request Size (KB):",
        options=avail_sizes,
        key="avail_sizes",
    )

    # Rename disks if more than one disk is found
    selected_disks = None
    disks_names = sorted(data["Disks"].unique().tolist())
    if len(disks_names) >= 1:
        # check if the user selected a disk to remove
        placeholder = sidebar.empty()
        selected_disks = placeholder.multiselect(
            label="Disks to display:",
            options=disks_names,
            default=st.session_state.selected_disks
            if "selected_disks" in st.session_state
            else disks_names,
            key="selected_disks",
        )
        if selected_disks:
            data = remove_disk(data, disks_names, selected_disks)

    # customize percentage threshold
    percentage_threshold = sidebar.slider(
        "Percentage threshold:",
        0.0,
        5.0,
        value=1.0,
        step=0.25,
        key="percentage_threshold",
    )

    # reset options/filters button
    sidebar.button("Clear Filters", on_click=reset_filters, args=(disks_names,))

    # check again if there is more than one disk (after removing disks)
    disks_names = sorted(data["Disks"].unique().tolist())
    if len(disks_names) >= 1:
        # check if user wants to rename disks
        st.header(":pencil: Rename your disks")
        new_names = {}
        for name in disks_names:
            new_names[name] = st.text_input(name + " name:", name, key=name)
        # update data with new names
        data = update_disks_names(data, new_names)

    # Check if the user selected a process name
    if filter_process != "ALL":
        data = data[data["Process Name"] == filter_process]

    # Check if the user selected a size to filter
    if filter_size != "ALL":
        data = data[data["Size (B)"] == filter_size * toKB]

    # Show log summary
    st.header(":page_facing_up: Data Summary")
    st.table(log_summary(data))

    # Plot summary
    plot_summary(data)

    st.header(":bar_chart: Request Size Charts")
    with st.expander("Show info"):
        st.write(
            """
            The requested size is the amount of data requested by an application when reading/writing from/to a file.
            Typically, the requested size is a multiple of the sector size.

            The most important SSD performance metric for you will depend on your usage pattern.
            The charts below show the average request size and the number of requests for each size.
            This, along with the previously shown read/write and random/sequential ratios,
            can help you determine the most important aspect of an SSD disk to consider for your desired workload.
            """
        )
    show_request_size_count(data)
    show_request_size_bytes(data)
    show_request_size_time(data)

    # Show access time info
    st.header(":stopwatch: Performance Charts")
    with st.expander("Show info"):
        st.write(
            """
            Access time refers to the duration (Disk Service Time) it requires to read or write a specific portion of a file.
            In other words, it is the time the disk spent to complete a request of a particular length (not considering the Windows I/O queue delay).

            I don't know why but access time reported by WPR is smaller than the access time measured by other benchmarking apps like CDM or AS SSD.
            """
        )
    show_performance(data, filter_size)


if __name__ == "__main__":
    # Increase pandas default output precision
    pd.set_option("display.precision", 7)
    pd.options.display.float_format = "{:.7f}".format

    # Set column names
    column_names = [
        "IO Type",
        "Priority",
        "Process Name",
        "Init Time (s)",
        "Complete Time (s)",
        "IO Time (µs)",
        "Disk Service Time (µs)",
        "Size (B)",
        "Min Offset",
        "Max Offset",
        "QD/I",
        "QD/C",
        "Disks",
        "Count",
    ]

    column_types = {
        "Count": int,
        "Disks": int,
        "QD/C": int,
        "QD/I": int,
        "Max Offset": str,
        "Min Offset": str,
        "Size (B)": str,
        "Disk Service Time (µs)": str,
        "IO Time (µs)": str,
        "Complete Time (s)": str,
        "Init Time (s)": str,
        "Process Name": str,
        "Priority": str,
        "IO Type": str,
    }

    # Define specific request sizes to account for
    length_values = [
        512,
        1024,
        2048,
        4096,
        8192,
        16384,
        32768,
        65536,
        131072,
        262144,
        524288,
        1048576,
        2097152,
        4194304,
    ]

    # drop rows with percentage below threshold
    percentage_threshold = 1.0

    # conversion factors from bytes
    toGB = 1024**3
    toMB = 1024**2
    toKB = 1024

    # conversion factors from microseconds
    toMSec = 1e3
    toSec = 1e6

    # Run the app
    t0 = timer()
    main()
    tf = timer()
    st.caption(f"Total computational time: {tf-t0:.2f} seconds.")
